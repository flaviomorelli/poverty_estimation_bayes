\section{Data transformation of the dependent variable}

At the unit level, income is usually a right-skewed distribution.
As measures of poverty and inequality are usually based on income, this characteristic shape poses challenges to model specification.
A classical linear regression with Gaussian errors will not be able to capture key aspects of the dependent variable due to its skewness.
A possible solution would be to use a regression with a skewed likelihood like a gamma, lognormal or skew-normal distribution.
In practice, those distributions have strong limitations: the logarithm of the lognormal has to be a normal distribution and the skewness of the skew-normal distribution is limited to the range $(-1, 1)$.
The inadequacy of these distribution to model income is shown in appendix ??? through posterior predictive checks.

\subsection{Log-shift transformation and skewness}
A common transformation for income in economic applications is the natural logarithm of the form $\log(y + \lambda)$. However, there might still be some skewness left, which is exacerbated by very low incomes.
If there are a lot of units with incomes between zero, there will likely be a long tail to the in the left side of the transformed distribution.
As \cite{rojas_perilla_data_2020} point out, it is possible to add a fixed term $s$ inside the logarithm so that $y+s \ge 1$ to avoid problems when $0 \le y \le 1$,
but the transformed variable might still be highly skewed.
Moreover, log-income is usually heavy-tailed.
This is not surprising, as a variable has to be distributed according to a log-normal distribution for its logarithm to be normally distributed.
As already mentioned, the log-normal distribution is usually not a good fit for income data.

To bring the dependent variable closer to a normal distribution, \cite{rojas_perilla_data_2020} explore different types of data-driven transformations such as Box-Cox or Yeo-Johnson.
While effective, these transformations are piecewise functions, which adds an additional layer of complexity when backtransforming to the original scale.
Another more simple data-driven transformation described by \cite{rojas_perilla_data_2020} is the log-shift defined as $y^* = \log(y + \lambda)$, where $y$ is the original variable and $\lambda$ is the shift term. Although $s$ and $\lambda$ fulfill similar purposes, they have different meanings: $s$ is a fixed term chosen in advance, whereas $\lambda$ is a parameter to be estimated.
By adjusting $\lambda$, it is possible to make the transformed variable more symmetric.
Moreover, the backtransformation is straighforward: $y = e^{y^*} - \lambda$.
However, the transformed variable might still have considerable excess kurtosis, even though it might still be almost symmetric.
\cite{rojas_perilla_data_2020} point out that minimizing skewness (Royston Lambert 2011) is just one approach to estimating $\lambda$.
One can also aim to minimize the distance (e.g. Kolomogorov-Smirnov or Cramér-von Mises) to another distribution, usually the Gaussian.
Their preferred approach is to maximize the REML of the model under data-transformations.
For the method proposed in the present paper, it is only necessary to minimize skewness to better fit the symmetric likelihood chosen for the model in the transformed scale.
Finally, note that the logarithm is a monotonic transform.
Therefore, this approach only works when the mode of the distribution in not at one extreme of the distribution support.
Specifically, this means that distributions that have a mode at their minimum like the exponential or Pareto will not become more symmetric after taking the logarithm and adding a shift term.

There is a further question related to dependent variable skewness in the linear mixed model.
\cite{rojas_perilla_data_2020} propose to a pooled skewness measure that weighs the skewness of the unit and area-level error according to their variances.
In principle, it is possible that skewness not only affects the unit-level error $\varepsilon_{di}$, but also the area-level error $u_{d}$.
While right-skewness is a common pattern of income at the unit-level (a few individuals/households earn much more than the rest), the picture is less clear at the area-level, as the areas can be defined in very different ways.
For example, if areas are defined as municipalities, there is a certain degree of arbitrariness to geographic boundaries:
although the Mexican states of Guerrero and Baja California have roughly the same area, the former has 81 municipalities while the latter only has six.
Any distribution of $u_d$ will reflect primarily the arbitrary subdivision rather than an underlying economic phenomenon.
Therefore, only the skewness at the unit-level errors is considered in this paper.
The Bayesian model proposed assumes that $u_d$ follows a normal distribution and is therefore symmetric by definition.

\subsection{Estimating the shift parameter}
A key question is how to estimate the shift term $\lambda$.
There are two options: estimate it from the data in an empirical Bayes way or do full Bayesian inference.
Both approaches have their advantages and disadvantages.
When estimating $\lambda$ from the data, the aim is to reduce skewness as much as possible.
This can be done by minimizing the absolute empirical skewness of $y^*$, or at least bringing it below a predetermined threshold.
This approach has the advantage that it is straightforward to control the skewness of $y^*$.
However, the uncertainty of estimating the shift parameter is not taken into account by not including the shift parameter into the model.

Integrating the shift parameter naively into the model might lead to estimation problems.
Defining the prior directly on $\lambda$ is not straightforward, as each different distributions might lead to  and with no further prior constraints the Markov chains might get stuck and not mix well.
As discussed in the previous section, minimizing the skewness of the transformed variable close to zero is likely to improve the performance of a symmetric likelihood distribution.
In the Bayesian context, this can be done by including a very tight prior around zero over skewness
\begin{equation*}
    S \sim \mathcal N(0, \delta).
\end{equation*}
Here, $\delta > 0$ is a small positive constant and $S$ is skewness defined as
\begin{equation*}
    \displaystyle S =  \frac{\frac 1 N \sum^{N}_{i = 0} (y_i^* - \bar y^* )^3}
    {\left[ \frac{1}{N - 1} \sum^{N}_{i = 0} (y_i^* - \bar y^* )^2 \right]^{3/2}},
\end{equation*}
where $y^* = \log(y + \lambda)$ is the transformed dependent variable. Thus, the prior on $S$ indirectly defines a prior on $\lambda$.
While it is not necessary to formulate a prior directly $\lambda$, it still recommended to define the lower bound of $\lambda$ as $-\min(y)$ in the programming framework as a safety check, as to avoid negative values inside the logarithm function.
In practice, the Markov chain steers clear of regions too close to the minimum for $\lambda$ after warmup iterations.

The hyperparameter $\delta$ controls the deviation from the zero skewness constraint.
Values around $10^{-4}$ worked well in simulation experiments and the posterior values for $S$ are usually very close to zero, but far beyond the implied 95\% prior region of $S \pm 2 10^{-4}$.
Nevertheless, there are two potential problems to be aware of.
If $\delta$ is too small, then skewness might be reduced too much.
This would be equivalent to overfitting the skewness of the training data, which by no means reflects the skewness for out-of-sample data.
However, if $\delta$ is too large, then many unrealistic values for $\lambda$ will be allowed, which might lead to poorly mixing Markov chains with a high R-hat (> 1.05).
By plotting the posterior density of $S$ and using Bayesian diagnostic tools such as R-hat or posterior predictive checks, it is possible to assess whether $\delta$ is too small or too large.

(EXAMPLES PLOTS for too large/small $\delta$?)

\subsection{Adjusting the Jacobian of the likelihood}

The transform of the dependent variable introduces a distortion that makes it necessary to adjust the Jacobian of the likelihood using Jacobi's transformation formula (12.6, Jacod Protter / Theorem 1.101 Klenke).
Let $X$ be a random variable defined over $I \subseteq \mathbb{R}$ with density $f^X$. For a continuous differentiable function $\varphi: I \rightarrow \mathbb{R}$ and $\varphi' \ne 0$ for all $x \in I$, define $Y := \varphi(X)$.
The density of the random variable $Y$ can then be defined as
\begin{equation*}
    f^Y(y) = f^X(\varphi^{-1}(y))
    \left|\displaystyle \frac{d \varphi^{-1}(y)}{dy}\right|
    \mathbb{I}(y \in \varphi(I)), ~~ y \in \mathbb{R},
\end{equation*}
where $\mathbb{I}(\cdot)$ is the indicator function.
Assuming that $y$ represents the dependent variable in the original scale, then $\varphi^{-1}(y) = \log(y + \lambda)$ and the Jacobian adjustment $\displaystyle \frac{d \varphi^{-1}(y)}{dy} = \frac{1}{y + \lambda}$, which is positive due to $y + \lambda > 0$. As the transformation parameter $\lambda$ is chosen to drastically reduce the skewness of $\log(y + \lambda)$, the density $f^X$ is chosen to be from a symmetric distribution – e.g., a Student's $t$-distribution. The likelihood of $y$ under the log-shift transformation is therefore
\begin{equation}
    \displaystyle f^Y(y) = \text{Student}(\log(y + \lambda)| \mu , \sigma, \nu)
    \cdot \frac{1}{y + \lambda}
    \cdot\mathbb{I}(y \ge \lambda), ~~ y \in \mathbb{R}.
\end{equation}

\subsection{Redifining the linear mixed model}

The Jacobian adjustment in the likelihood due to the log-shift transformation leads to a reformulation of model XY as follows:


