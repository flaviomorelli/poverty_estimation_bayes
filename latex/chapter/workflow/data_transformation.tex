\subsection{Alternative 1: Data-driven transformations}
\label{ch:log_shift}

As discussed by \cite{rojas_perilla_data_2020}, data-driven transformations can change the distribution shape of the dependent variable so that it is more convenient to use simple methods such as a linear mixed model with a Gaussian error term.
While \cite{rojas_perilla_data_2020} also includes a discussion of multiple data-driven transformations such as the Box-Cox or the Yeo-Johnson, the focus here lies on the log-shift transform, because it is closest to the conceptually simple logarithmic transform.
This section starts with a general discussion of the log-shift transform and then shows how to include the transform in the Bayesian model.

\subsubsection{Log-shift transformation and skewness}
A common transformation for income in economic applications is the natural logarithm of the form $\log(y + \lambda)$. However, there might still be some skewness left, which is exacerbated by very low incomes.
If there are a lot of units with incomes between zero, there will likely be a long tail to the in the left side of the transformed distribution.
As \cite{rojas_perilla_data_2020} point out, it is possible to add a fixed term $s$ inside the logarithm so that $y+s \ge 1$ to avoid problems when $0 \le y \le 1$,
but the transformed variable might still be highly skewed.
Moreover, log-income is usually heavy-tailed.
This is not surprising, as a variable has to be distributed according to a log-normal distribution for its logarithm to be normally distributed.

To bring the dependent variable closer to a normal distribution, \cite{rojas_perilla_data_2020} explore different types of data-driven transformations such as Box-Cox or Yeo-Johnson.
While effective, these transformations are piecewise functions, which adds an additional layer of complexity when backtransforming to the original scale.
Another more simple data-driven transformation described by \cite{rojas_perilla_data_2020} is the log-shift defined as $y^* = \log(y + \lambda)$, where $y$ is the original variable and $\lambda$ is the shift term. Although $s$ and $\lambda$ fulfill similar purposes, they have different meanings: $s$ is a fixed term chosen in advance, whereas $\lambda$ is a parameter to be estimated.
A key advantage is that the backtransformation is straighforward: $y = e^{y^*} - \lambda$.
However, the transformed variable might still have considerable excess kurtosis, even though it might still be almost symmetric.
By adjusting $\lambda$, it is possible to make the transformed variable more symmetric.
\cite{rojas_perilla_data_2020} point out that minimizing skewness is just one approach to estimating $\lambda$.
One can also aim to minimize the distance (e.g. Kolomogorov-Smirnov or Cramér-von Mises) to another distribution, usually the Gaussian.
Their preferred approach is to maximize the REML of the model under data-transformations.
For the method proposed in this section, it is only necessary to minimize skewness to better fit the symmetric likelihood chosen for the model in the transformed scale.

There is a further question related to dependent variable skewness in the linear mixed model.
\cite{rojas_perilla_data_2020} propose to a pooled skewness measure that weighs the skewness of the unit and area-level error according to their variances.
In principle, it is possible that skewness not only affects the unit-level error $\varepsilon_{di}$, but also the area-level error $u_{d}$.
While right-skewness is a common pattern of income at the unit-level (a few individuals/households earn much more than the rest), the picture is less clear at the area-level, as the areas can be defined in very different ways.
For example, if areas are defined as municipalities, there is a certain degree of arbitrariness to geographic boundaries:
although the Mexican states of Guerrero and Baja California have roughly the same area, the former has 81 municipalities while the latter only has six.
Any distribution of $u_d$ will reflect primarily the arbitrary subdivision rather than an underlying economic phenomenon.
Therefore, only the skewness at the unit-level errors is considered in this paper.
The Bayesian model proposed assumes that $u_d$ follows a normal distribution and is therefore symmetric by definition.

\subsubsection{Estimating the shift parameter}
A key question is how to estimate the shift term $\lambda$.
There are two options: estimate it from the data in an empirical Bayes way or do full Bayesian inference.
Both approaches have their advantages and disadvantages.
When estimating $\lambda$ from the data, the aim is to reduce skewness as much as possible.
This can be done by minimizing the absolute empirical skewness of $y^*$, or at least bringing it below a predetermined threshold.
This approach has the advantage that it is straightforward to control the reduction of skewness in $y^*$.
However, the uncertainty of estimating the shift parameter is not taken into account, as the shift parameter is not included into the model.
In practice, the empirical Bayes that minimizes skewness should be in the same range of the full Bayesian estimate, so it can be used as an additional check for the Bayesian model.

Integrating the shift parameter $\lambda$ naively into the model might lead to estimation problems.
Defining the prior directly on $\lambda$ is not straightforward, as each different distributions might lead to different result and with no further prior constraints the Markov chains might get stuck and not mix well.
As discussed in the previous section, minimizing the skewness of the transformed variable close to zero is likely to improve the performance of a symmetric likelihood distribution.
In the Bayesian context, this can be done by including a very tight prior on skewness centered around zero
\begin{equation*}
    S \sim \mathcal N(0, \delta).
\end{equation*}
Here, $\delta > 0$ is a small positive constant and $S$ is skewness defined as
\begin{equation*}
    \displaystyle S(y^*) =  \frac{\frac 1 N \sum^{N}_{i = 0} (y_i^* - \bar y^* )^3}
    {\left[ \frac{1}{N - 1} \sum^{N}_{i = 0} (y_i^* - \bar y^* )^2 \right]^{3/2}},
\end{equation*}
where $y^* = \log(y + \lambda)$ is the transformed dependent variable. Thus, the prior on $S$ indirectly defines a prior on $\lambda$.
While it is not necessary to formulate a prior directly $\lambda$, it still recommended to define the lower bound of $\lambda$ as $-\min(y)$ in the programming framework as a safety check, as to avoid negative values inside the logarithm function.
In practice, the Markov chain steers clear of regions too close to the minimum for $\lambda$ after warmup iterations.



Because of the transformation, the Jacobian of the likelihood is adjusted by the multiplicative factor $(y_{di} + \lambda)^{-1}$, derived in appendix \ref{appendix:jacobian}. Together with the prior on skewness, this leads to a reformulation of model \ref{eq:mod_hb} as follows:

\begin{equation}
    \begin{split}
        p(\log(y_{di} + \lambda) |\boldsymbol \beta, u_d, \sigma_e, \nu)   =        \text{Student}&(\log(y_{di} + \lambda)| \boldsymbol{x'}_{di} \boldsymbol \beta + u_d,\ \sigma_e\ , \nu)\cdot (y_{di} + \lambda)^{-1}, \\
        u_d | \sigma_u & \sim \mathcal N(0, \sigma_u),\ d = 1, ..., D \\
        \beta_k & \sim \mathcal N(\mu_k, \sigma_k),\ k = 1, ..., K\\
        \sigma_u & \sim Ga(2, 0.75), \\
        \sigma_e & \sim Ga(2, 0.75), \\
        \nu & \sim Ga(2, 0.1), \\
        S(\log(y_{di} + \lambda)) & \sim \mathcal N(0, \delta), ~ \delta > 0\\
    \end{split}
    \label{eq:trafo_hb}
\end{equation}
where $\ d = 1, ..., D,\ i = 1, ..., N_d$. The transformation $\log(y_{di} + \lambda)$ is left explicitly in the model to remind the reader that the prior on skewness directly affects lambda.

After a log-shift transformation the regression coefficients can be interpreted as an approximate percentage change in the original scale.
A detailed justification for this interpretation can be found in appendix \ref{appendix:coeff}.
Note that the use of the exponential distribution in the backtransformation together with the heavy-tailed Student's $t$-distribution can lead to very extreme prediction in the backtransformed scale.
Empirically, the percentage of predictions that above the dependent variable maximum is between 0.03\% and 0.5\%.
This proportion is very small and should not have a large impact on the poverty indicators, as they depend on the median, which is robust to outliers.
Nevertheless, it can cause problems when examining the samples from the posterior predictive distribtuion.
Therefore, all sample that are above the data maximum are taken as missing and are imputed.
The imputation procedure is explained in detail in appendix \ref{ch:imputation}.



The hyperparameter $\delta$ controls the deviation from the zero skewness constraint.
Values around $10^{-2}$ worked well in simulation experiments and the posterior values for $S(y^*)$ are usually very close to zero.
Nevertheless, there are two potential problems to be aware of.
If $\delta$ is too small, then skewness might be reduced too much.
This would be equivalent to overfitting the skewness of the training data, which by no means reflects the skewness for out-of-sample data.
However, if $\delta$ is too large, then many unrealistic values for $\lambda$ will be allowed, which might lead to poorly mixing Markov chains with an $\hat R$ higher than the recommended 1.01.
By plotting the posterior density of $S(y^*)$ and using Bayesian diagnostic tools such as posterior checks, it is possible to assess whether $\delta$ is too small or too large.


\begin{figure}[t]
    \centering
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\textwidth]{./graphics/log_shift/bad_skewness}
        \caption{$S(y^*) \sim \mathcal N (0, 0.1)$}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\textwidth]{./graphics/log_shift/mid_skewness}
        \caption{$S(y^*) \sim \mathcal N (0, 0.01)$}
    \end{subfigure}
    \begin{subfigure}{0.32\linewidth}
        \includegraphics[width=\textwidth]{./graphics/log_shift/gb2_skewness}
        \caption{$S(y^*) \sim \mathcal N (0, 0.001)$}
    \end{subfigure}

    \caption[Marginal posterior distribution of skewness for different values of $\delta$.]{Marginal posterior distribution of skewness $S(y^*)$ for different values of $\delta$ in the GB2 scenario: 0.1, 0.01 and 0.001. Note the different scaling of the x-axes.}
    \label{fig:bad_skewness}
\end{figure}

Figure \ref{fig:bad_skewness} shows the density plots of $S(y^*)$ for the GB2 scenario.
The other scenarios displayed similar results.
For $\delta = 0.1$, the mode of the posterior is below zero, which indicates that there is a systematic bias towards negative skewness.
This is problematic when using a symmetric likelihood.
On the other extreme, $\delta = 0.001$ produces an extremely tight distribution around zero.
The range of posterior values for $\lambda$ is very small in this specification, which increases the risk of overfitting the training data.
The middle density graph ($\delta = 0.01$) is a compromise between the two extreme scenarios.
The posterior distribution for $S(y^*)$ is clearly centered around zero, but the posterior is not as tight as with $\delta = 0.001$.
The standard deviation of the shift parameter $\lambda$ is around an order of magnitude larger than in the model with the tighter prior, which allows more possible models and prevents overfitting.
Moreover, the elpd$_{\text{loo}}$ paints



\subsubsection{Posterior predictive checks with simulated data}
The results from fitting the modified model to the log-shift distribution are presented in Figure \ref{fig:ppc_logshift}.
In the first column, the density of the dependent variable is overlaid with the densities from 100 sample from the posterior predictive distribution.
From this virst visual check, it is clear that the model is able to fit the log-scale and GB2 scenarios reasonably well.
The Pareto scenario is still quite close, but the prediction from the model are slightly lower than the dependent variable.
This is an indication that the Pareto scenario contains elements that are particularly challenging to capture.


\begin{figure}
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\linewidth]{./graphics/log_shift/logscale_smp_}
        \caption{Log-scale}
    \end{subfigure}
    \newline
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\linewidth]{./graphics/log_shift/gb2_smp_}
        \caption{GB2}
    \end{subfigure}
    \newline
    \begin{subfigure}{\textwidth}
        \includegraphics[width=\linewidth]{./graphics/log_shift/pareto_smp_}
        \caption{Pareto}
    \end{subfigure}
    \caption[Posterior predictive check for the log-shift model with all three simulation scenarios.]{Posterior predictive check for the log-shift model with all three simulation scenarios. \textit{Left:} density of the dependent variable (black) against the  density of a 100 backtransformed predictions (light blue). \textit{Middle:} scatterplot of IQR against median for 1000 samples. \textit{Right:} scatterplot of standard deviation against mean for 1000 samples. In the middle and right columns, the dark point represents the respective values for the dependent variable in the original data set.}
    \label{fig:ppc_logshift}
\end{figure}

The other two columns show two pairs of descriptive statistics as scatterplots: IQR against median and standard deviation against the mean.
The scatterplots also display the respective descriptive statistic value for the dependent variable.
In the log-scale and GB2 scenarios, the median of the simulations is in a similar range to the median of the dependent variable.
In contrast, the predictions from the model have a median that is 400 units lower than the dependent variable of the Pareto scenario.
This would mean that the simulated poverty lines (ca. 60\% of 11.900) are systematically lower than the poverty line from the data (ca. 60\% of 12.300) – approximately by 3\%.
Thus, certain poverty indicators are likely to be underestimated.
On the other hand, all samples from the posterior predictive distribution in the three scenarios are in a plausible range compared to the IQR of the dependent variable.
The mean and the standard deviation are presented as an additional check, but they are somewhat less trustworthy due to their lack of robustness.
In the logscale scenario, both the mean and the standard deviation are captured well, whereas in the GB2 scenario the mean and standard deviation of the predictions is a little lower than the dependent variable.
The standard deviation Pareto scenario is modelled well, but the mean of the dependent variable is higher than the predictions.
A noteworthy pattern is that in all three scenarios there is a positive association between the mean an the standard deviation.
Although the probability density for the backtransformation is not available in its analytical form, this is a clear indication that the mean of the implied distribution is coupled to the variance in the backtransformed scale.
The next section explores the performance of models with skewed likelihoods.

