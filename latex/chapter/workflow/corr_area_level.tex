\section{Modelling correlations at the area-level}
\label{ch:area_corr}

Until now, the assumption has been that all random effects at the area-level are independent from each other, i.e., $u_d|\sigma_u \sim \mathcal N (0, \sigma_u)$.
However, this assumption seems unplausible.
For example, if the areas are defined as municipalities, it is likely that neighbouring regions have similar characteristics and are therefore correlated.
If on the other hand the areas are the strata considered in the previous section, the 16 strata that correspond to a rural area have more in common with each other than with urban strata.
In this section, a new prior that captures correlations at the area level is introduced to the model.
The result is then compared to the model that was defined in the previous section.
Thus, the domains are not defined as the municipalities, but as the strata presented in the last section.

There are multiple options to model correlation between areas.
The simpler LKJ prior based on the work by \cite{lewandowski_generating_2009} places a restriction over permissible correlation matrices.
Autoregressive approaches such as IAR, CAR and SAR \citep{chung_bayesian_2020} represent another way of capturing spatial correlation and takes into account how similar or close the different domains are.
Moreover, other priors such as the random walk prior used used in \cite{gao_improving_2021} offers an alternative to modelling dependencies between areas.

The resulting model will be compared with model \ref{eq:log_scenario} that assumes independence between areas, but with the modified random effect.
A comparison between all possible priors that capture area correlation is beyond the scope of this paper and left for future research.

\subsection{LKJ prior}

Let $\Sigma$ be a $D \times D$ covariance matrix, so that $u \sim \mathcal{N}(\boldsymbol{0}_D, \Sigma)$, where $\boldsymbol 0_D$ and $u$ are $D$-dimensional vectors.
The density of $u$ can be written as:
\begin{gather*}
    p(u|\Sigma) = (2\pi)^{-\frac D 2}\det(\Sigma)^{-\frac 1 2} e^{(-\frac 1 2 u'\Sigma^{-1} u)}.
\end{gather*}
The covariance matrix $\Sigma$ can be decomposed as $\Sigma = \text{diag}(\tau)\Omega\text{diag}(\tau)$, where $\tau$ is a $D$-dimensional vector of scale factor, $\text{diag}(\tau)$ is a $D \times D$ matrix with $\tau$ as its main diagonal and $\Omega$ is a $D \times D$ correlation matrix.
Up to this point, the assumption has been that there is only one scale parameter $\sigma_u$ for all area-level effects $u_d$ and this assumption is still kept.
Because $\tau_d = \sqrt{\Sigma_{d, d}}, d = 1, ..., D$, the decomposition is simplified to $\Sigma = \sigma_u^2 \Omega$.

The standard deviation for the random effect $\sigma_u^2$, already has the prior $Ga(2, 7)$.
However, the correlation between domains is captured by defining a LKJ prior over the correlation matrix $\Omega$\footnote{In practice, it is common to use the Cholesky decomposition of $\Omega$ to avoid numerical issues when estimating the model.
For clarity, the traditional matrix notation is kept in the paper, but note that the decomposition is used in the accompanying code.}:
\begin{gather*}
    \Omega \sim \text{LKJ}(\eta), ~~ \eta \ge 1.
\end{gather*}
The LKJ correlation distribution is defined so that $p(\Omega|\eta) \propto \det(\Omega)^{\eta - 1}$ \citep[Chapter 1.13]{stan_development_team_stan_2021}.
Note that the determinant of $\Omega$ increases as the correlation between components decreases:
an identity matrix has a determinant of 1, while a correlation matrix consisting only of ones (perfect correlation between components) has a determinant of 0.
For $\eta = 1$, the LKJ prior is a uniform distribution over all possible correlation matrices.
However, due to the fact that the function $f(\eta) = k^{\eta-1}$ for a fixed $k \in (0, 1)$ does not converge uniformly towards zero as $\eta \rightarrow \infty$, a higher $\eta$ puts more probability mass on matrices with a higher determinant, i.e., on matrices with a lower correlation between components.
In the model, $\eta$ is set to 5, which strongly favors matrices with determinants closer to 1, i.e., correlation matrices with a moderate correlation between domains.
Figure \ref{fig:heatmap_lkj} shows four posterior draws for $\Omega$ as heatmaps.
There are correlation values in the range from -0.5 to 0.5, which indicates that there is some correlation between areas.
However, the heatmaps do not reveal any clear correlation pattern.
This will be discussed more in detail in section \ref{ch:comparison_lkj_sar}.
In the next step, the SAR prior is presented and included into the model as an alternative to the LKJ prior.

%The results from the model comparison can be seen in in Table XY.
%The model that allows for correlation between areas has a slightly better performance according to PSIS-LOO than the model with no correlation at all.
%However, it is important to remember that elpd$_{\text{LOO}}$ is just an approximation, because PSIS-LOO quantifies predictive power at the unit-level and not at the area-level, whereas the aim of the model is to generate prediction at the area-level.
%It is likely that the difference between the predictive power of both specifications would be clearer when using leave-one-group-out cross-validation.
%Unfortunately, this is still an area of active research and it is therefore beyond the scope of this paper.
%In any case, the slightly better performance of the model with the LKJ priors is taken as a sign that it captures correlations between strata that improve predictions.
%Eta parameter?





\subsection{SAR prior}
The simultaneous autoregression (SAR) prior as described in \cite{chung_bayesian_2020} is defined on a precision matrix $\Pi$.
The starting point is the $D\times D$ proximity matrix $W$, which contains information on how close two domains are.
If $w_d, d = 1, ..., D$ is defined as the sum of row $d$ of matrix $W$ – i.e., $w_d = \displaystyle \sum_{i = 1}^D w_{di}$ –, then the $D \times D$ matrix $L$ is defined as $L = \text{diag}\{w_d\}_{d=1}^D$.
Thus, it is possible to calculate the row-normalized matrix $\widetilde W = L^{-1}W$, in which all rows sum to 1.
The SAR prior in the context of model \ref{eq:trafo_coef_var} is then given by
\begin{gather*}
    \Pi(\rho) = (I_D - \rho \widetilde W)'(I_D - \rho \widetilde W), ~~ \rho \in (-1, 1),\\
    u \sim \mathcal N(\boldsymbol{0}_D, \sigma_u^2 \Pi^{-1}),\\
    \rho \sim \mathcal U(-1, 1),
\end{gather*}
where $\boldsymbol{0}_D$ is a $D$-dimensional zero vector.
Again, note that in this model $\Pi$ is \textit{precision} and not a \textit{correlation} matrix.
Because the single strata used as domains are coded as a 5-digit binary string (see Section \ref{ch:raneff}), the distance matrix $W$ is defined using the Hamming distance.
In short, the Hamming distance takes two strings of the same length and counts how many digits are different, e.g., 00111 and 10100 have a Hamming distance of 3.
This also ensures that the distance of each stratum from itself is 0.
The maximum distance in this case is 5, because each stratum is code with five binary digits.
Note that when $\rho = 0$ the precision matrix $\Pi$ is equal to the identity matrix, which means that there is no correlation between the domains.
As $\rho$ deviates from 0, the effect of the spatial increases.
The uniform prior is chosen for $\rho$, because it is not possible to know how strong the correlation values are before fitting the model to the data.
The limits $\rho \in (-1, 1)$ are needed so that $\Pi$ is positive definite.
Similarly to the LKJ prior, Figure \ref{fig:heatmap_sar} shows four posterior draws for the correlation matrix.
Here, the actual correlation matrix is depicted, \textit{not} the precision matrix $\Pi$.\footnote{$\Pi^{-1}$ is a covariance matrix, which is rescaled with the \code{cov2cor} function in \code{R} to get the correlation matrix. The scaling parameter $\sigma^2_u$ is ignored in the rescaling as it is only a multiplicative constant of the covariance matrix.}
The pattern created by the distance matrix is very clear and will be discussed more in detail in the next section.
Finally, although the prior on $\rho$ is uniform, its posterior distribution (not shown) is right skewed, with 70\% of the probability mass between -1 and 0 and the rest between 0 and 1.
It is not straightforward to interpret the effect of this parameter for large matrices, as the precision matrix $\Pi$ has to be inverted at a later stage.
However, a clear deviation from 0 (as in this case) indicates area-level correlation.

\begin{figure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.85\textwidth]{./graphics/rand_intercept/lkj_corr_plot}
        \caption{LKJ(5) prior}
        \label{fig:heatmap_lkj}
    \end{subfigure}
    \begin{subfigure}{\linewidth}
        \centering
        \includegraphics[width=0.85\textwidth]{./graphics/rand_intercept/sar_corr_plot}
        \caption{SAR prior}
        \label{fig:heatmap_sar}
    \end{subfigure}
    \caption[Correlation matrices for the LKJ and SAR priors.]{Four samples of the posterior correlation matrices for the models with LKJ and SAR priors. The domain coding in the axes is explained in section \ref{ch:raneff}.}
    \label{fig:corr_heatmap}
\end{figure}

\subsection{Comparison of LKJ and SAR priors}
\label{ch:comparison_lkj_sar}
The main difference between both correlation priors is that the SAR prior uses actual information on how similar or close the domains are.
In contrast, the LKJ prior is only a prior on the admissible correlation matrices.
The difference becomes clear in Figure \ref{fig:corr_heatmap}, which shows four posterior samples of the correlation matrices from the models with the LKJ and SAR priors.
The axes display the respective domains, whose notation as a 5-digit binary string (e.g., 01100) was explained in section \ref{ch:raneff}.
The correlation matrices produced by the LKJ prior display a completely random pattern with respect to the correlations.
On the othe side, with the SAR prior there is a clear structure in the correlation matrices stemming from the distance matrix $W$.
Even though in Figure \ref{fig:heatmap_map} the correlation values vary from sample to sample, there is a very clear symmetric visible for each sample, which is related to the way Hamming distance used to define $W$.\footnote{Figure \ref{fig:corr_heatmap} depicts only four posterior samples from more than 1000 MCMC samples. The correlation pattern in the SAR prior is present in all samples, but the range of correlation values from just four posterior samples is not representative.}
The nature of the pattern can be further understood in Figure \ref{fig:corr_density}, which shows the distributions of the correlation values for each one of the posterior samples in Figure \ref{fig:corr_heatmap}.
The LKJ prior does not force any particular correlation pattern on the data, but it guarantees that the distribution of the correlation values is roughly equal for each sample.
With an increasing $\eta$, the LKJ prior puts more weight on correlation matrices closer to the identity matrix, which leads to tighter distributions for the correlation values.
On the other hand, there is a much clearer pattern in the densities of the SAR prior specifications.
The spikes in the densities are cause by the Hamming distance, which in the present paper has takes only six values, from 0 to 5.
With continuous distances, the densities would be much smoother.
Moreover, the correlation values densities do not overlap as in the LKJ prior.

\begin{figure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=0.85\textwidth]{./graphics/rand_intercept/lkj_dens_plot}
        \caption{LKJ(5) prior}
    \end{subfigure}
    \begin{subfigure}{0.49\linewidth}
        \includegraphics[width=0.85\textwidth]{./graphics/rand_intercept/sar_dens_plot}
        \caption{SAR prior}
    \end{subfigure}
    \caption[Correlation density for the LKJ and SAR priors]{Correlation density for the four posterior samples of the LKJ and SAR priors shown in Figure \ref{fig:corr_density}. Each sample are displayed in a different color.}
    \label{fig:corr_density}
\end{figure}


The LKJ and SAR specification are compared with model \ref{eq:trafo_coef_var} without domain-level correlation (base model) using PSIS-LOO.
All models use the stratified random effect specification.
The results are shown in Table \ref{tab:lkj_sar_base}.
The elpd$_{\text{loo}}$ of the SAR model is slightly worse than the base model (-0.07), but the standard error of the difference (second column) is more than three times the estimated elpd$_{\text{loo}}$ difference itself (0.24).
According to PSIS-LOO, the base and SAR models are undistinguishable.
The elpd$_{\text{loo}}$ difference for the LKJ specification (-0.72) is more than three times its standard error (0.22).
Neverteheless, it is important to notice that the elpd$_{\text{loo}}$ (third column) are in an almost identical range and that the standard errors are quite large.
Therefore, when looking at the larger picture, the differences between models seem to be marginal.



The main question at this point is why there is no clearer difference between these three specifications.
There are three possible explanations.
First, as some of the areas are quite small, it is difficult to indentify correlations between areas.
It is possible that with more data the correlation could become more visible.
Second, the PSIS-LOO quantifies leave-one-out cross-validation.
However, leaving out only one observation might not make a large difference for the correlations are at the area-level.
PSIS-LOO might not be the right tool to quantify the difference and alternatives such as leave-one-group-out cross-validation could provide a clearer result.
Third, the predictors might have a good predictive power so that area-level correlation does not have such a large impact.
Finally, this result points to a blind spot in the simulation scenarios, because they do not take area-level correlation into account.
Therefore, it is not possible to know for certain, whether the similar results for all three models in Table \ref{tab:lkj_sar_base} is due to lack of data, to the limitations of PSIS-LOO or simply because the area-level correlation is very small.
Simulated data could have been used to check whether the diagnostic tools can capture a difference in predictive power due to area-level correlations.
Unfortunately, due to this limitations of the simulation scenarios from section \ref{ch:simulations} it is not possible to know the exact reason for the similarity between the three model specifications.
A critical discussion of the simulations scenarios can be found in section \ref{ch:adequacy_simulations}.

% latex table generated in R 4.0.5 by xtable 1.8-4 package
% Fri Aug 20 12:38:58 2021
\begin{table}[ht]
    \centering
    \caption{Comparison of LKJ, SAR and base specification with PSIS-LOO.}
    \begin{tabular}{rrrrrrrrr}
        \hline
        & elpd$_{\text{loo}}$ diff. & S.E. diff. & elpd$_{\text{loo}}$ & S.E. elpd$_{\text{loo}}$  \\
        \hline
        Base & 0.00 & 0.00 & -14799.32 & 54.40  \\
        SAR & -0.07 & 0.24 & -14799.39 & 54.45  \\
        LKJ & -0.72 & 0.22 & -14800.05 & 54.40  \\
        \hline
    \end{tabular}

    \label{tab:lkj_sar_base}
\end{table}


In the current model specification, the random intercept was redifined so that there are no out-of-sample areas.
Nevertheless, the LKJ and SAR priors are not as useful when there are out-of-sample regions, as the correlation matrix is assumed to have as many dimensions as in the training set.
In such cases, an autoregressive or a random walk prior on $u_d$ such as in \cite{gao_improving_2021} can be more appropriate, as it does not depend strictly on the dimensions of the correlation matrix.
In any case, it is implausible that the area-level effects are completely independent.
Therefore, the SAR specification is chosen as the final model.
Next section compares all models fitted until now using stacking weights.













