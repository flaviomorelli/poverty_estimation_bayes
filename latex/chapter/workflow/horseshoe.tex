\section{Variable selection}
\label{ch:varsel}
A key step in specifying the model is deciding which variables to use.
However, this is at the same time one of the more challenging steps.
The ideal solution would be to estimate models with all possible variable combinations and then compare the predictive quality of the models – e.g., through cross-validaton.
However, this solution is not feasible computationally due to the very large number of possible models.
Another common approach is to use shrinkage methods like the Lasso \citep{tibshirani_regression_1996} to determine which variables can be left out without significantly compromising the predictive power of the model.
If a coefficient is shrunk to zero, it is a sign that the respective variable might be removed.
In this section, the regularized horseshoe prior \citep{piironen_sparsity_2017}, which follows a shrinkage principle similar to the Lasso or the ridge, is presented and then used to select relevant variables.
This section shows how priors can be used inside a Bayesian workflow to do variable selection.
The selection is done by comparing the elpd$_{\text{loo}}$ of models with different groups of less meaningful variables removed. An alternative to the horseshoe prior is to use the projective prediction approach developed by \cite{piironen_projective_2020}.
Unfortunately, this only works with distributions from the exponential family, which does not include the Student's $t$-distribution used in this paper.
Therefore, the focus lies on the regularized horseshoe prior.

\subsection{Regularized horseshoe prior}

In the Bayesian framework, shrinkage can be best understood in relation to the prior.
The general idea is to have a narrow region of high density around zero that shrinks coefficients,
while at the same time including fat tails to allow some coefficients to deviate from zero over a wider range.
In its most extreme formulation, it corresponds to the spike-and-slab prior introduced originally by \cite{mitchell_bayesian_1988}.
While this prior has a high theoretical relevance, it can be computationally demanding for a large number of variables and is sensitive to specification details such as slab width.
On the other hand, continuous shrinkage priors provide similar results while also being easier to compute \citep{piironen_sparsity_2017}.
The horseshoe prior \citep{carvalho_horseshoe_2010} is a continuous shrinkage prior with a similar performance to the spike-and-slab prior and can be formulated as
\begin{equation}
    \begin{split}
        \theta_j | \lambda_j, \tau & \sim \mathcal N (0, \tau^2\lambda^2_j), \\
        \lambda_j & \sim \mathcal C^+ (0, 1), ~~ j = 1,..., J,
    \end{split}
    \label{eq_hs}
\end{equation}
where $\theta_j$ are the parameters to be shrunk and $\mathcal C^+$ is the half-Cauchy distribution. In a regression setting, $\theta_j$ corresponds to the coefficients $\beta_j$.
This prior is characterized by a global parameter $\tau$ that shrinks all parameters towards zero as $\tau \rightarrow 0$ and a local parameter $\lambda_j$ for each $\theta_j$ that pushes against the global shrinkage.
Due to the extremely heavy tails of $\mathcal C^+(0, 1)$, some $\lambda_j$ will be large enough to counteract even small values of $\tau$.

However, there is no consensus on how to do inference for $\tau$ and parameters far from zero are not shrunk at all.
\cite{piironen_sparsity_2017} address this issue by introducing the regularized horseshoe prior, which is given by
\begin{equation}
    \begin{split}
        \theta_j | \lambda_j, \tau & \sim \mathcal N (0, \tau^2 \tilde \lambda^2_j),
        ~~ \tilde \lambda_j^2 = \displaystyle \frac{c^2 \lambda^2_j}{c^2 + \tau^2 \lambda_j^2}\\
        \lambda_j & \sim \mathcal C^+ (0, 1), ~~ j = 1,..., J,
    \end{split}
\end{equation}
where $c > 0$ is a constant.
If $\tau^2\lambda^2_j \ll c^2$, then $\theta_j$ is shrunk strongly towards zero and $\tilde \lambda^2_j \rightarrow \lambda^2_j$, which approximates the original horseshoe in expression \ref{eq_hs}.
For the case $\tau^2\lambda^2_j \gg c^2$, then $\tilde \lambda^2_j \rightarrow c^2/\tau^2$ and the prior reduces to $\mathcal N(0, c^2)$, which corresponds to a Gaussian slab with variance $c^2$ that regularizes $\theta_j$.
When $c \rightarrow \infty$ the regularizing effect of the prior disappears, just as in the original horseshoe.
Although $c$ can be chosen as a constant, \cite{piironen_sparsity_2017} recommend doing inference with the prior
\begin{equation*}
    c^2 \sim \text{Inv-Gamma}(\alpha, \beta), ~~ \alpha = \nu/2, ~~ \beta = \nu s^2/2,
\end{equation*}
which leads to a Student$(0, s^2, \nu)$ slab for $\theta_j$ far away from zero.
This is desirable as the heavier tails of the Student's $t$ distribution prevent too much probability mass accumulating near zero.

The last parameter in the specification of the regularized horseshoe is $\tau$.
For a linear model, \cite{piironen_sparsity_2017} explore different specifications for $\tau$.
Defining $p_0$ as the prior guess of the number of non-zero variables, the prior for $\tau$ should have most of its mass near
\begin{equation}
    \displaystyle \tau_0 = \frac{p_0}{J-p_0}\frac{\sigma}{\sqrt n},
    \label{eq_tau0}
\end{equation}
where $\sigma$ is the variance of the error term\footnote{In \cite{piironen_sparsity_2017}, the assumed linear model does not have a random intercept. Thus the randomness comes mostly from the unit-level error term $\varepsilon \sim \mathcal N (0, \sigma)$.}.
However, there is no best choice for the prior of $\tau$. The three main options are setting $\tau = \tau_0$, or assuming either $\tau \sim \mathcal C^+(0, \tau_0^2)$ or $\tau \sim \mathcal N^+(0, \tau_0^2)$, where $\mathcal{N^+}$ is the half-normal distribution.
In practice, it is common to set the ratio of relevant and irrelevant parameters, – $p_0/(J - p_0)$ –, and use $\tau = \tau_0$.
In this paper, a ratio of 0.2 is chosen, which assumes that for every 10 irrelevant regressors, there are 2 relevant variables.
Note that the definition of $\tau_0$ in equation \ref{eq_tau0} is derived under the assumption of a Gaussian model.
Therefore, its meaning is only an approximation when using the Student's $t$-distribution.

Certain shrinkage methods like Lasso force certain coefficients to be zero, which provides an easy selection criterion.
However, a disadvantage of the horseshoe is that it does not have an integrated criterion to select variables. There are two possible workarounds for this problem.
Firstly, one could leave out variables whose mean coefficient is below a certain threshold \citep{piironen_sparsity_2017}.
Secondly, one can leave out variables if a significant portion of the coefficient distribution is in both a positive and a negative area.
To check that the predictive power of the model does not deteriorate by removing variables, PSIS-LOO can be used to compare the full model with the smaller model.

\subsection{Results of variable selection and further considerations}
The package \code{brms} was used to avoid implementing the horseshoe prior from scratch.
As the package does not provide data-driven transformations, the model is estimated with a simple logarithmic transformation of income and a Student's $t$-likelihood.
While this does not correspond exactly to the log-shift model presented in the previous section, it can be seen as an approximation.
The shift term is unlikely to have a major impact on the predictive power of the regressors.
This can be clearly seen in the backtransformation ($e^\eta - \lambda$, with $\eta$ the linear predictor), where the shift term is just an additive constant shifting $e^\eta$.
The original horseshoe prior does not take into account the grouping of categorical variables.
In this case, if any of the categories has a clear effect on the dependent variable, the whole variable is considered to be relevant.

After estimating the full model with the horseshoe prior, the following variables presented in section \ref{ch:mexican_data} have either very small effects or effects that are not clearly positive or negative: job experience of head of household, gender of head of household, presence of minors under the age of 16, percentage of working household members, percentage of women in household and percentage of literate members of household.
The density plots of the coefficients can be found in Figure \ref{fig:hs_dens} in appendix \ref{appendix:horseshoe_plot}.
A reduced model without these six variables but with the same prior and likelihood specification as the full model was estimated in a second step, but the difference in elpd$_{\text{loo}}$ to the full model is -17.8 with a standard error for the difference of 7.3.
The difference is therefore more than twice as large as its standard error, so it reduces the predictive power of the model.
A second model with only gender of head of household, percentage of women in household and percentage of literate members of household removed is indistinguishable from the full model with an elpd$_{\text{loo}}$ difference of -2.7 and a difference standard error of 4.2.
Therefore, the variables selected in the second model are used for the rest of the paper.

Before moving to the next section, two issues need to be addressed.
First, the effectiveness of the horseshoe prior depends crucially on the number of variables included in the full model.
The shrinkage will be most noticeable with a large number of variables.
Otherwise, the shrinkage from the horseshoe is very similar to placing a prior centered around zero for the coefficients.
In this project, ca. 20 variables were chosen from the survey and census data.
Therefore, the shrinkage of the regularized horseshoe is not much different from the shrinkage of a non-structured prior (e.g., independent normal priors on the regression coefficients).
In any case, this section showed that it is possible to select variables using the horseshoe prior and that its shrinkage properties are particularly useful in high-dimensional data sets.

Finally, finding an adequate subset of variables is a field of active research in the context of Bayesian workflow.
The projective predictive distribution approach provides a clear selection of variables that keeps the predictive power of the full model, but at the moment of writing only available for the exponential family \citep{piironen_sparsity_2017}.
As the $t$-distribution is one of the likelihoods used in this paper, no projective predicitve methods are used in this paper.
It is very likely that in the near future there will be new tools to do this step of the workflow, especially concerning the projective predictive method to distributions outside the exponential family.

