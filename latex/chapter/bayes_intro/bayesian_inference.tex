\subsection{Bayesian inference and terminology}

Inference is the process of analyzing population parameters based on samples from the population.
Because the sample is an imperfect representation of the population, there is always uncertainty associated with parameter inference.
In frequentist statistics, the common approach is to calculate point estimates of the true parameters from the data.
Uncertainty about the estimate comes from the randomness of the data in the sample and it is usually quantified by confidence intervals, which are based on distributional assumptions such as normality.
Bayesian inference takes a different approach: the true parameter is not a constant, but a random variable with a distribution, while the data is assumed to be fixed.
The aim is to estimate the distribution of the population parameter $\theta$ given the data $y$, i.e. $p(\theta|y)$.
The derivation of the distribution is based on Bayes' theorem
\begin{gather*}
	\displaystyle p(\theta | y) \overset {\text{(a)}}{=}  \frac {p(\theta, y)}{p(y)} \overset {\text{(b)}}{=} \frac {p(\theta) p(y|\theta)}{\int p(\theta) p(y|\theta)d\theta}.
\end{gather*}
Here, $\theta$ is a vector of parameters and vector $y$ represents the data, which can be anything from a single variable to a regression design matrix with its respective outcome variable.
Inference is based on the distribution $p(\theta | y )$ – the \textbf{posterior distribution}.
The first equality (a) is the definition of conditional probability.
By using the definition of conditional probability again, it is possible to decompose the joint distribution $p(\theta, y)$ into $p(\theta)$ and $p(y|\theta)$ in the second inequality (b).
$p(\theta)$ is the \textbf{prior distribution} which encodes knowledge about the parameters prior to collecting the data. One example of prior information in the context of a socio-economic survey would be knowing the past average per capita income or the Gini coefficient of a country from sources such as a textbook or existing research papers before even collecting current data.
Prior information also includes well-known facts about distribution parameters – e.g. the variance cannot be negative.
This information can be used to parametrize the prior distribution.
The \textbf{likelihood} is given by $p(y | \theta)$ and it reflects the modelling of the data generating process.
A simple example is a binomial distribution that models the number of sucesses $y$ in $n$ tosses of a coin, given the probability $\theta$.
Both the prior and the likelihood are the main ingredients of Bayesian inference.

The term in the denominator $p(y)$ is the \textbf{marginal likelihood} which gets its name from marginalizing $\theta$ out of the expression in the numerator by integration: $\int p(\theta) p(y|\theta)d\theta$.
The marginal likelihood is a normalizing constant that ensures that $p(\theta|y)$ is a proper distribution, i.e. integrates to 1.
The integral is intractable even for very simple models and analytic solutions exist mostly for some cases of conjugate priors.
Therefore, it is necessary to use Monte Carlo methods or variational inference to estimate the posterior (see appendix A). The popularity of Bayesian methods has increased with the availabilty of computational power that makes such estimation methods more accessible.
Nevertheless, the marginal likelihood is more than a simple normalizing constant.
By sampling from $p(y)$, the marginal likelihood can be used to generate data from the model, even before observing any data.
Therefore, it is also called the \textbf{prior predictive distribution}.
To generate new data  $\tilde y$ from the posterior model, replace the prior $p(\theta)$ with the posterior $p(\theta|y)$ in the definition of $p(y)$ and the result is $p(\tilde y|y) = \int p(y | \theta) p(\theta|y) d\theta$, where $p(\tilde y | y)$ is the \textbf{posterior predictive distribution}.
Data generated from the prior predictive and posterior predictive distributions can be used to check model quality.
A more detailed description of the concepts presented in this section can be found in chapters 1 and 2 of \cite{gelman_bayesian_2014}.

