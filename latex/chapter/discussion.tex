\chapter{Discussion}

After presenting the results, it is necessary to evaluate them critically and also to revisit certain steps of the workflow that could potentially be improved.
In the first section, the Bayesian and EBP are compared in their advantages and disadvantages.
The second section discusses whether a Bayesian workflow is useful in the context of survey statistics and more specifically of small area estimation.
Lastly, the simulation scenarios are evaluated with respect to their limitations and extensions to these scenarios are proposed.


\section{Critical evaluation of the Bayesian approach against the EBP}

The aim of this paper was to develop a Bayesian model for poverty estimation iteratively.
However, it is puzzling that the estimated indicators from the final HB model are lower than the EBP results.
There are a number of possible reasons for the discrepancy:
\begin{itemize}
    \item \textit{Regularization}: prior distributions have a regularizing effect on the model, while the EBP does not contain any form of shrinkage.
    \item \textit{Random effect specification}: the stratified random effect specification from section \ref{ch:raneff} might lead to systematically lower estimates than the traditional BHF specification.
    \item \textit{Survey weights}: by not taking into account the survey weights,– through which one observation can count as multiple observations –, a Bayesian model can have trouble capturing the true distributional characteristics of the data.
\end{itemize}
It is noteworthy that the EBP and HB estimates are almost identical, when using the survey weights with the HB model in the calculation of FGT indicators (not shown).
\textit{Area-level correlation} was excluded as a cause for the discrepancy between the two models, as the estimated indicators from the base model without spatial correlation and from the SAR model are almost identical.

The HB estimates were already lower than the EBP estimates in \cite{morelli_hierarchical_2021}, which used the traditional random effect based on municipalities.
Therefore, the discrepancy is unlikely to be caused only by the stratified random effect specification.
Moreover, the simulation study in section \ref{ch:rmse_hb} showed that even when using a Bayesian model with proper priors that induce shrinkage, the estimates for the indicators are almost identical.
These observations lead us to the following conclusion.
On the one hand, the simulation scenarios are constructed in a way that is unlikely to lead to overfitting.
However, shrinkage in the Bayesian model might be more noticeable when using more complex real-world data, which might include covariates that increase the risk of overfitting.
On the other hand, survey weights bring the HB model in line with the EBP model and this indicates that they might play a key role in adjusting the distributional characteristics of the Bayesian model.
Further research is needed to answer this question conclusively.
For this, it is necessary to take into account both overfitting and survey weights in the simulation scenario.
An additional check could include estimates from a skewed distribution such as the gamma with a log link to see whether the HB estimates change substantially.
Finally, as the EBP and HB models use different uncertainty measures and have different random effect specifications, it is more difficult to compare them with respect to estimate uncertainty.
Nevertheless, avoiding out-of-sample regions by redefining the random effect was shown to reduce estimate uncertainty drastically and it is plausible that it would have a similar effect on the EBP.

An additional dimension to consider is ease of use.
The \code{emdi} package already provides a series of implemented models out of the box.
In contrast, the models developed in this paper were all coded using \code{Stan}, which requires more specialized knowledge of how Bayesian models are computed.
This can be overwhelming for some users.
Many modelling decisions such as prior choice and estimation of FGT-indicators from backtransformed samples from the posterior predictive distribution can be automatized in a package like \code{brms}.
Moreover, it is important to keep computation time in mind.
The EBP model took around 8 minutes to fit \textit{and} produce various socio-economic indicators such as HCR and PGAP.
The final Bayesian model with the SAR prior also takes around 8 minutes to fit, but an additional 5 to 10 minutes are needed to calculate the indicators, depending on the number of Monte Carlo samples.
While this still is an acceptable and realistic time, it is important to underline that Bayesian models usually take longer to estimate and that model complexity can have a huge impact in this respect.
Moreover, the \code{Stan} code was optimized for speed by taking full advantage of vectorization and slight changes can lead to a large increase in computational time.

\section{Using a Bayesian workflow for SAE}

This section discusses the benefits and disadvantages of Bayesian workflow.
A key advantage of using a Bayesian workflow is that any model changes can be presented and analyzed in a transparent way.
There is a variety of tools that can be used to check model assumptions (prior predictive checks), model fit (posterior predictive checks), or predictive power (PSIS-LOO and stacking).
Moreover, as the model parameters are distributions and not point estimates it is always possible to look at each parameter distribution independently to check if the values are realistic and also to better understand how the model works.
This can be done both for unidimensional parameters such as standard deviation and multidimensional parameters such as correlation matrices (e.g., Figure \ref{fig:corr_heatmap}).
In summary, there are tools to understand how the model works and the iterative development of the model allows for new insights into the problem at hand and makes the uncertainty in model choice visible.
This transparency is well-suited for survey statistics – a field which often provides information for public policy decision-making.

Nevertheless, there are still disadvantages in using a Bayesian workflow.
As Bayesian statistics has become more widespread only in the last few years, there are still some tools in need of development.
Variable selection is one of those areas, as the horseshoe prior is still not ideal for this task due to the lack of a clear selection criterion and due to its tendency to create computation problems like HMC divergences.
A better alternative would be the projective prediction approach from \cite{piironen_projective_2020}, but which at the time of writing only works for a few likelihoods from the exponential family.
This is an area of active research and new developments are to be expected in the following years.
Another difficult aspect of Bayesian workflow is prior choice, especially for groups of similar parameters such as regression coefficients.
While prior predictive checks are useful to determine the impact of prior choice, there might be too many possibilities to be considered.
A current area of research is joint priors such as the R2D2 \citep{zhang_bayesian_2020} or even the regularized horseshoe \citep{piironen_sparsity_2017}, which affects multiple parameters simultaneously.
Joint priors are simpler to parametrize and are therefore more user-friendly.
They can also have advantages in avoiding overfitting, as are usually controlled by parameters such as $R^2$ or number of relevant parameters.
Lastly, as \cite{gelman_bayesian_2020} point out, iterative model fitting might cause problems with respect to inference validity.
Again, this is an area of current research.

\section{Adequacy of simulation scenarios}
\label{ch:adequacy_simulations}

The use of simulated data is a central part of Bayesian workflow according to \cite{gelman_bayesian_2020}.
However, it is also a time-consuming step.
In general, it is hard to create a scenario or multiple scenarios that capture the main features of the data, while at the same time not defining overly complicated scnearios or generating so many scenarios that it is a burden for the researcher to work with them.

The models used in this paper are an extension of the models in \cite{rojas_perilla_data_2020}, mainly through the addition of correlated covariates.
Nevertheless, there are some critical aspects that can be reconsidered.
First, there might be room for improving the assumed distributions in the scenarios.
On the one hand, the GB2 and Pareto scenarios add a skewed error term to covariates and random effects that are normal.
The result is a relatively symmetric distribution with a long right tail.
To make both scenarios more realistic, the distributions of the regressors could be more varied, so as to avoid a strong symmetry before adding the error term.
On the other hand, the log-scale scenario assumes normality in the logarithmic scale.
While this is a good sanity check for the results, it is unlikely to reflect the difficulty of fitting real-world data.
A simple modification to avoid this strong assumption, would be again to change the normal distribution of the regressors to a heavier tailed Student's $t$ with different the degrees of freedom for each covariate.
Second, a majority of variables found in a survey or a census are categorical.
Neither \cite{rojas_perilla_data_2020} nor the present paper include categorical variables in the simulations.
Therefore, there are probably some blind spots in the analysis of this paper, regarding difficulties that might arise from the use of categorical covariates.
Third, the area-level effects are assumed to be independent – an unrealistic assumption.
Many different ways of including correlation were already mentioned in section \ref{ch:area_corr}: an LKJ prior, an SAR prior or using a random walk between areas.
While choosing any one of those procedures can place a strong assumption on the simulations, it should at least help the researcher to show whether the model can capture correlations at the area level even with small areas.
Finally, this paper ignored the issue of problematic regressors.
The correlation between covariates is moderate, only 0.2.
Fitting a model that does not overfit is more challenging with a higher correlation between regressors and also with covariates that are highly correlated with the dependent variable.
Including these suggestions into the simulations scenarios, can provide a more realistic setting to test the models.

At the same time, it is crucial not to overcomplicate the models and not to have too many scenarios.
In this paper, the GB2 and the Pareto scenarios are both additive and generate dependent variables that are similar in shape.
To simplify model analysis, it might be easier to limit the scenarios to one multiplicative scenario and one additive scenario, either with a GB2 or a Pareto error.

One last aspect to take into account is the sampling procedure used with the simulated data.
In this paper, the survey data set was created by sampling from the simulated population with no out-of-sample domains.
However, there are cases where many domains are out-of-sample.
For example, in many states of the Mexican data set, there no observations for 50\%-70\% of domains in the survey.
This is an area of future research, especially when comparing the differences between EBP and an HB model with informative priors.


