\chapter{Appendix: Coefficient Interpretation after Log-Shift Transformation}
\label{appendix:coeff}
The transformation of the dependent variable complicates coefficient interpretation.
In a prediction context, coefficients are of secondary importance, as the estimated values might be bias due to complex interactions between variables, e.g., omitted variable bias, post-treatment bias, collider bias.
Models with biased coefficient can offer even better predictive performance than non-biased models (MCELREATH).
However, in a Bayesian context it is still useful to understand how regressors affect the dependent variable to choose adequate priors.

For simplicity, consider a regression with a transformed dependent variable and two independent variables:
\begin{equation*}
    \log(y + \lambda) = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon
\end{equation*}
By taking the exponential function on both sides, the expression becomes
\begin{equation*}
    y + \lambda = e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon}
\end{equation*}
Assuming that $\lambda$ is a fixed constant, it can be analyzed how $y$ changes when increasing $x_1$ by one unit
\begin{equation}
    \displaystyle \frac{y^{new} + \lambda}{y^{old} + \lambda} =
    \frac{e^{\beta_0 + \beta_1 (x_1 + 1) + \beta_2 x_2 + \varepsilon}}
    {e^{\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \varepsilon}} = e^{\beta_1}.
\end{equation}
When $\lambda = 0$, this derivation corresponds to the coefficient interpretation in a log-linear model as an approximate percentage change (SOURCE – Stock \& Watson?).
This stems from the approximate relation $\beta_1 \approx e^{\beta_1} - 1$, which is accurate for small $\beta_1$ ($e^{0.04} - 1 \approx 0.0408$), but worsens as $\beta_1$ increases ($e^{0.6} - 1 \approx 0.822$).
Nevertheless, this inaccuracy has to be put into perspective.
If $x_1$ is scaled so that an increase of one unit is comparatively small, an increase of 4\% in income for each additional unit of $x_1$ seems plausible and even high.
On the other hand, $\beta_1 = 0.6$ would imply an increase of around 82\%, which seems extraordinarily high for a comparatively small change in $x_1$.
Therefore, it is safe to expect that most coefficients in a log-linear should be rather close to zero – assuming that there is no confounding that might radically change the magnitude of coefficients.

Finally, the effect of the shift term $\lambda$ on coefficient interpretation has to be considered.
There are two main factors.
First, the distortion of the interpretation of $\beta_1$ is smaller as $\lambda$ gets closer to zero or is comparatively small compared to the whole range of $y$.
As $\lambda$ is usually below the 2\% quantile of variable $y$ (s. simulation results in CHAPTER XY), it is reasonable to assume that the interpretation as an approximate percentage change can still be used.
Second, it is crucial to remember that a linear regression is mainly related to the expected value of the dependent variable.
$\beta_1$ has to be interpreted with this in mind.
Focusing on extreme regions of $y$, shows a larger effect of $\lambda$.
For example, assume that $y \in (0, 100)$ and $\lambda = 1$.
For values near the minimum of $y$, $\lambda$ has a very large influence: $\frac{y^{new} + 1 } {y^{old} + 1} = \frac{3 + 1}  {2 + 1} = 1.33$, while with no shift term $\lambda$ the ratio between $y^{new}$ and $y^{old}$ would be 1.5. With larger values of $y$ the difference between the ratios of shifted and non-shifted dependent variables is much smaller: $\frac{30 + 1}{20 + 1} = 1.48$, which is much closer to $\frac{30}{20} = 1.5$.

In summary, the traditional coefficient interpretation as approximate percentage changes in a log-linear model can also be used with a log-shift transform.
The interpretation has some caveats that assume that $\lambda$ is small compared to the range of $y$ and that the focus is not on values in the extremes of $y$.
However, the interpretation when $\lambda = 0$ is itself an approximation and it is therefore possible to use it as a rule of thumb when $\lambda \ne 0$.