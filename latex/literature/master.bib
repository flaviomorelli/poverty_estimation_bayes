
@book{gelman_bayesian_2014,
	address = {Boca Raton, FL (US)},
	edition = {3},
	title = {Bayesian {Data} {Analysis}},
	publisher = {CRC Press},
	author = {Gelman, A and Carlin, J B and Stern, H S and Dunson, D B and Vehtari, A and Rubin, D B},
	year = {2014},
	keywords = {high priority},
	file = {Gelman et al. - Bayesian Data Analysis, Third Edition.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/EQN8H782/Gelman et al. - Bayesian Data Analysis, Third Edition.pdf:application/pdf},
}

@book{jacod_probability_2004,
	address = {Berlin, Heidelberg},
	edition = {2},
	title = {Probability {Essentials}},
	publisher = {Springer-Verlag},
	author = {Jacod, Jean and Protter, Philip},
	year = {2004},
	file = {Jacod and Protter - 2004 - Probability Essentials.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/FG7FV47G/Jacod and Protter - 2004 - Probability Essentials.pdf:application/pdf},
}

@book{mcelreath_statistical_2020,
	address = {Boca Raton, FL (US)},
	edition = {2},
	title = {Statistical {Rethinking}: {A} {Bayesian} {Course} with {Examples} in {R} and {Stan}},
	publisher = {CRC Press},
	author = {McElreath, Richard},
	year = {2020},
	file = {McElreath - 2018 - Statistical Rethinking A Bayesian Course with Exa.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/ZFSRKEWS/McElreath - 2018 - Statistical Rethinking A Bayesian Course with Exa.pdf:application/pdf;statisticalrethinking2_06Feb2020.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/9XQB7XMF/statisticalrethinking2_06Feb2020.pdf:application/pdf},
}

@book{stock_introduction_2015,
	address = {Harlow, England},
	edition = {3},
	title = {Introduction to {Econometrics}},
	publisher = {Pearson Education Limited},
	author = {Stock, James H. and Watson, Mark W.},
	year = {2015},
	file = {Stock, James H._ Watson, Mark W. - Introduction to Econometrics-Pearson Education (2015).pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/RGGW5W2Y/Stock, James H._ Watson, Mark W. - Introduction to Econometrics-Pearson Education (2015).pdf:application/pdf},
}

@article{betancourt_conceptual_2017,
	title = {A {Conceptual} {Introduction} to {Hamiltonian} {Monte} {Carlo}},
	url = {http://arxiv.org/abs/1701.02434},
	abstract = {Hamiltonian Monte Carlo has proven a remarkable empirical success, but only recently have we begun to develop a rigorous understanding of why it performs so well on diﬃcult problems and how it is best applied in practice. Unfortunately, that understanding is conﬁned within the mathematics of diﬀerential geometry which has limited its dissemination, especially to the applied communities for which it is particularly important.},
	urldate = {2021-01-21},
	author = {Betancourt, Michael},
	year = {2017},
	keywords = {statistics, bayesian statistics, monte carlo, hamiltonian, bayesian computation, markov chain},
	file = {Betancourt - 2017 - A Conceptual Introduction to Hamiltonian Monte Car.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/SAI2Q3CZ/Betancourt - 2017 - A Conceptual Introduction to Hamiltonian Monte Car.pdf:application/pdf},
}

@article{hoffman_no-u-turn_2014,
	title = {The {No}-{U}-{Turn} {Sampler}: {Adaptively} {Setting} {Path} {Lengths} in {Hamiltonian} {Monte} {Carlo}},
	volume = {15},
	abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by ﬁrst-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC’s performance is highly sensitive to two user-speciﬁed parameters: a step size and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS performs at least as eﬃciently as (and sometimes more eﬃciently than) a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter on the ﬂy based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all, making it suitable for applications such as BUGS-style automatic inference engines that require eﬃcient “turnkey” samplers.},
	language = {en},
	journal = {Journal of Machine Learning Research},
	author = {Hoffman, Matthew D and Gelman, Andrew},
	year = {2014},
	keywords = {statistics, bayesian statistics, monte carlo, hamiltonian, bayesian computation, markov chain, nuts},
	pages = {1593--1623},
	file = {Hoﬀman and Gelman - The No-U-Turn Sampler Adaptively Setting Path Len.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/V4D23DPZ/Hoﬀman and Gelman - The No-U-Turn Sampler Adaptively Setting Path Len.pdf:application/pdf},
}

@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	shorttitle = {Variational {Inference}},
	abstract = {One of the core problems of modern statistics is to approximate difﬁcult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to ﬁrst posit a family of densities and then to ﬁnd the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-ﬁeld variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
	language = {en},
	number = {518},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	year = {2017},
	pages = {859--877},
	file = {Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/FM9U7DTH/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf:application/pdf},
}

@article{blei_latent_2003,
	title = {Latent {Dirichlet} {Allocation}},
	volume = {3},
	language = {en},
	journal = {Journal of Machine Learning Research},
	author = {Blei, David M and Ng, Andrew Y. and Jordan, Michael I.},
	year = {2003},
	pages = {993--1022},
	file = {Blei - Latent Dirichlet Allocation.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/44N8NUA5/Blei - Latent Dirichlet Allocation.pdf:application/pdf},
}

@book{rao_small_2015,
	address = {Hoboken, NJ (USA)},
	edition = {2},
	title = {Small {Area} {Estimation}},
	publisher = {John Wiley \& Sons, Inc},
	author = {Rao, J. N. K. and Molina, Isabel},
	year = {2015},
	keywords = {statistics, textbook, sae},
	file = {(Wiley Series in Survey Methodology) J.N.K. Rao, Isabel Molina - Small Area Estimation-Wiley (2015).pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/KEQG98YJ/(Wiley Series in Survey Methodology) J.N.K. Rao, Isabel Molina - Small Area Estimation-Wiley (2015).pdf:application/pdf},
}

@misc{gelman_prior_2020,
	title = {Prior {Choice} {Recommendations}},
	url = {https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations},
	urldate = {2021-08-06},
	author = {Gelman, Andrew},
	year = {2020},
	keywords = {bayesian statistics, stan, priors},
}

@article{battese_error_1988,
	title = {An error components model for prediction of county crop areas using survey and satellite data},
	volume = {83},
	journal = {Journal of the American Statistical Association},
	author = {Battese, G. E. and Harter, R. M. and Fuller, W. A.},
	year = {1988},
	pages = {28--36},
	file = {Battese et al. - 1982 - An error components model for prediction of county.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/MENWQHZ6/Battese et al. - 1982 - An error components model for prediction of county.pdf:application/pdf},
}

@article{molina_small_2014,
	title = {Small area estimation of general parameters with application to poverty indicators: {A} hierarchical {Bayes} approach},
	volume = {8},
	number = {2},
	journal = {The Annals of Applied Statistics},
	author = {Molina, Isabel and Nandram, Balgobin and Rao, J. N. K.},
	year = {2014},
	pages = {852--885},
	file = {Molina et al. - 2014 - Small area estimation of general parameters with a.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/MHMEEHD2/Molina et al. - 2014 - Small area estimation of general parameters with a.pdf:application/pdf},
}

@article{vehtari_practical_2017,
	title = {Practical {Bayesian} model evaluation using leave-one-out cross-validation and {WAIC}},
	volume = {27},
	abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a ﬁtted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an eﬃcient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the ﬁnite case with weak priors or inﬂuential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models ﬁt with the Bayesian inference package Stan.},
	number = {5},
	journal = {Statistics and Computing},
	author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
	year = {2017},
	keywords = {statistics, bayesian statistics, model evaluation},
	pages = {1413--1432},
	file = {Vehtari et al. - 2017 - Practical Bayesian model evaluation using leave-on.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/9CTQ9VLC/Vehtari et al. - 2017 - Practical Bayesian model evaluation using leave-on.pdf:application/pdf},
}

@article{tzavidis_start_2018,
	title = {From start to finish: a framework for the production of small area official statistics},
	volume = {181},
	abstract = {Small area estimation is a research area in ofﬁcial and survey statistics of great practical relevance for national statistical institutes and related organizations. Despite rapid developments in methodology and software, researchers and users would beneﬁt from having practical guidelines for the process of small area estimation. We propose a general framework for the production of small area statistics that is governed by the principle of parsimony and is based on three broadly deﬁned stages, namely speciﬁcation, analysis and adaptation, and evaluation. Emphasis is given to the interaction between a user of small area statistics and the statistician in specifying the target geography and parameters in the light of the available data. Model-free and model-dependent methods are described with a focus on model selection and testing, model diagnostics and adaptations such as use of data transformations. Uncertainty measures and the use of model and design-based simulations for method evaluation are also at the centre of the paper. We illustrate the application of the proposed framework by using real data for the estimation of non-linear deprivation indicators. Linear statistics, e.g. averages, are included as special cases of the general framework.},
	number = {4},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Tzavidis, Nikos and Zhang, Li‐Chun and Luna, Angela and Schmid, Timo and Rojas Perilla, Natalia},
	year = {2018},
	keywords = {statistics, sae},
	pages = {927--979},
	file = {Tzavidis et al. - 2018 - From start to finish a framework for the producti.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/T2LUV8IS/Tzavidis et al. - 2018 - From start to finish a framework for the producti.pdf:application/pdf},
}

@article{rojas_perilla_data_2020,
	title = {Data driven transformations in small area estimation},
	volume = {183},
	abstract = {Small area models typically depend on the validity of model assumptions. For example, a commonly used version of the empirical best predictor relies on the Gaussian assumptions of the error terms of the linear mixed regression model: a feature rarely observed in applications with real data. The paper tackles the potential lack of validity of the model assumptions by using data-driven scaled transformations as opposed to ad hoc chosen transformations. Different types of transformations are explored, the estimation of the transformation parameters is studied in detail under the linear mixed regression model and transformations are used in small area prediction of linear and non-linear parameters. The use of scaled transformations is crucial as it enables ﬁtting the linear mixed regression model with standard software and hence it simpliﬁes the work of the data analyst. Mean-squared error estimation that accounts for the uncertainty due to the estimation of the transformation parameters is explored by using the parametric and semiparametric (wild) bootstrap. The methods proposed are illustrated by using real survey and census data for estimating income deprivation parameters for municipalities in the Mexican state of Guerrero. Simulation studies and the results from the application show that using carefully selected, data-driven transformations can improve small area estimation.},
	number = {1},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Rojas Perilla, Natalia and Pannier, Sören and Schmid, Timo and Tzavidis, Nikos},
	year = {2020},
	keywords = {statistics, sae, data transformation},
	pages = {121--148},
	file = {Rojas‐Perilla et al. - 2020 - Data‐driven transformations in small area estimati.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/UYIF493Q/Rojas‐Perilla et al. - 2020 - Data‐driven transformations in small area estimati.pdf:application/pdf},
}

@article{vehtari_rank-normalization_2021,
	title = {Rank-{Normalization}, {Folding}, and {Localization}: {An} {Improved} {R}-hat for {Assessing} {Convergence} of {MCMC}},
	volume = {16},
	number = {2},
	journal = {Bayesian Analysis},
	author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and Bürkner, Paul-Christian},
	year = {2021},
	pages = {667--718},
	file = {Vehtari et al. - 2020 - Rank-Normalization, Folding, and Localization An .pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/Y9PMX46A/Vehtari et al. - 2020 - Rank-Normalization, Folding, and Localization An .pdf:application/pdf},
}

@article{foster_class_1984,
	title = {A {Class} of {Decomposable} {Poverty} {Measures}},
	volume = {52},
	number = {3},
	journal = {Econometrica},
	author = {Foster, James and Greer, Joel and Thorbecke, Erik},
	year = {1984},
	keywords = {econometrics, poverty estimation},
	pages = {761--766},
	file = {Foster et al. - 1984 - A Class of Decomposable Poverty Measures.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/HR5WBGWI/Foster et al. - 1984 - A Class of Decomposable Poverty Measures.pdf:application/pdf},
}

@misc{r_core_team_r_2021,
	title = {R: {A} {Language} and {Environment} for {Statistical} {Computing}},
	url = {https://www.R-project.org},
	urldate = {2021-08-05},
	author = {{R Core Team}},
	year = {2021},
}

@article{paananen_implicitly_2021,
	title = {Implicitly {Adaptive} {Importance} {Sampling}},
	volume = {31},
	abstract = {Adaptive importance sampling is a class of techniques for ﬁnding good proposal distributions for importance sampling. Often the proposal distributions are standard probability distributions whose parameters are adapted based on the mismatch between the current proposal and a target distribution. In this work, we present an implicit adaptive importance sampling method that applies to complicated distributions which are not available in closed form. The method iteratively matches the moments of a set of Monte Carlo draws to weighted moments based on importance weights. We apply the method to Bayesian leave-one-out cross-validation and show that it performs better than many existing parametric adaptive importance sampling methods while being computationally inexpensive.},
	number = {16},
	journal = {Statistics and Computing},
	author = {Paananen, Topi and Piironen, Juho and Bürkner, Paul-Christian and Vehtari, Aki},
	year = {2021},
	keywords = {bayesian statistics, model evaluation},
	file = {Paananen et al. - 2020 - Implicitly Adaptive Importance Sampling.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/B69INYGW/Paananen et al. - 2020 - Implicitly Adaptive Importance Sampling.pdf:application/pdf},
}

@misc{encyclopaedia_britannica_guerrero_2019,
	title = {Guerrero},
	url = {https://www.britannica.com/place/Guerrero},
	urldate = {2021-01-21},
	author = {{Encyclopaedia Britannica}},
	year = {2019},
}

@article{box_science_1976,
	title = {Science and {Statistics}},
	volume = {71},
	number = {356},
	journal = {Journal of the American Statistical Association},
	author = {Box, George},
	year = {1976},
	pages = {791--799},
	file = {Box - 1976 - Science and Statistics.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/P5Z9GT65/Box - 1976 - Science and Statistics.pdf:application/pdf},
}

@techreport{chung_bayesian_2020,
	address = {Washington, D.C. (US)},
	title = {Bayesian hierarchical spatial models for small area estimation},
	institution = {U.S. Census Bureau},
	author = {Chung, Hee Cheol and Datta, Gauri Sankar},
	year = {2020},
	file = {Chung and Datta - Bayesian hierarchical spatial models for small are.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/Q63I54VS/Chung and Datta - Bayesian hierarchical spatial models for small are.pdf:application/pdf},
}

@article{gelman_bayesian_2020,
	title = {Bayesian {Workflow}},
	url = {https://arxiv.org/abs/2011.01808},
	urldate = {2021-09-10},
	author = {Gelman, Andrew and Vehtari, Aki and Simpson, Daniel and Margossian, Charles C. and Carpenter, Bob and Yao, Yuling and Kennedy, Lauren and Gabry, Jonah and Bürkner, Paul-Christian and Modrák, Martin},
	year = {2020},
	keywords = {bayesian statistics},
	file = {Bayesian_Workflow_article.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/U2BGLWFL/Bayesian_Workflow_article.pdf:application/pdf},
}

@article{chakraborty_robust_2019,
	title = {Robust {Hierarchical} {Bayes} {Small} {Area} {Estimation} for the {Nested} {Error} {Linear} {Regression} {Model}},
	volume = {87},
	issn = {0306-7734, 1751-5823},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12283},
	doi = {10.1111/insr.12283},
	abstract = {Standard model-based small area estimates perform poorly in presence of outliers. Sinha \& Rao (2009) developed robust frequentist predictors of small area means. In this article, we present a robust Bayesian method to handle outliers in unit-level data by extending the nested error regression model. We consider a ﬁnite mixture of normal distributions for the unit-level error to model outliers and produce noninformative Bayes predictors of small area means. Our modelling approach generalises that of Datta \& Ghosh (1991) under the normality assumption. Application of our method to a data set which is suspected to contain an outlier conﬁrms this suspicion, correctly identiﬁes the suspected outlier and produces robust predictors and posterior standard deviations of the small area means. Evaluation of several procedures including the M-quantile method of Chambers \& Tzavidis (2006) via simulations shows that our proposed method is as good as other procedures in terms of bias, variability and coverage probability of conﬁdence and credible intervals when there are no outliers. In the presence of outliers, while our method and Sinha–Rao method perform similarly, they improve over the other methods. This superior performance of our procedure shows its dual (Bayes and frequentist) dominance, which should make it attractive to all practitioners, Bayesians and frequentists, of small area estimation.},
	language = {en},
	number = {S1},
	urldate = {2021-03-12},
	journal = {International Statistical Review},
	author = {Chakraborty, Adrijo and Datta, Gauri Sankar and Mandal, Abhyuday},
	month = may,
	year = {2019},
	keywords = {bayesian statistics, sae},
	pages = {S158--S176},
	file = {Chakraborty et al. - 2019 - Robust Hierarchical Bayes Small Area Estimation fo.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/NJQ6KE54/Chakraborty et al. - 2019 - Robust Hierarchical Bayes Small Area Estimation fo.pdf:application/pdf},
}

@article{piironen_comparison_2017,
	title = {Comparison of {Bayesian} predictive methods for model selection},
	volume = {27},
	abstract = {The goal of this paper is to compare several widely used Bayesian model selection methods in practical model selection problems, highlight their differences and give recommendations about the preferred approaches. We focus on the variable subset selection for regression and classiﬁcation and perform several numerical experiments using both simulated and real world data. The results show that the optimization of a utility estimate such as the cross-validation (CV) score is liable to ﬁnding overﬁtted models due to relatively high variance in the utility estimates when the data is scarce. This can also lead to substantial selection induced bias and optimism in the performance evaluation for the selected model. From a predictive viewpoint, best results are obtained by accounting for model uncertainty by forming the full encompassing model, such as the Bayesian model averaging solution over the candidate models. If the encompassing model is too complex, it can be robustly simpliﬁed by the projection method, in which the information of the full model is projected onto the submodels. This approach is substantially less prone to overﬁtting than selection based on CV-score. Overall, the projection method appears to outperform also the maximum a posteriori model and the selection of the most probable variables. The study also demonstrates that the model selection can greatly beneﬁt from using crossvalidation outside the searching process both for guiding the model size selection and assessing the predictive performance of the ﬁnally selected model.},
	number = {3},
	journal = {Statistics and Computing},
	author = {Piironen, Juho and Vehtari, Aki},
	year = {2017},
	keywords = {bayesian statistics, model evaluation},
	pages = {711--735},
	file = {Piironen and Vehtari - 2017 - Comparison of Bayesian predictive methods for mode.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/NRVQDL23/Piironen and Vehtari - 2017 - Comparison of Bayesian predictive methods for mode.pdf:application/pdf},
}

@article{gabry_visualization_2019,
	title = {Visualization in {Bayesian} workflow},
	volume = {182},
	abstract = {Bayesian data analysis is about more than just computing a posterior distribution, and Bayesian visualization is about more than trace plots of Markov chains. Practical Bayesian data analysis, like all data analysis, is an iterative process of model building, inference, model checking and evaluation, and model expansion. Visualization is helpful in each of these stages of the Bayesian workﬂow and it is indispensable when drawing inferences from the types of modern, high dimensional models that are used by applied researchers.},
	language = {en},
	number = {2},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
	year = {2019},
	keywords = {data visualization, statistics, bayesian statistics},
	pages = {389--402},
	file = {Gabry et al. - 2019 - Visualization in Bayesian workflow.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/BIFCDUNP/Gabry et al. - 2019 - Visualization in Bayesian workflow.pdf:application/pdf},
}

@article{sinha_robust_2009,
	title = {Robust small area estimation},
	volume = {37},
	issn = {03195724, 1708945X},
	url = {http://doi.wiley.com/10.1002/cjs.10029},
	doi = {10.1002/cjs.10029},
	language = {en},
	number = {3},
	urldate = {2021-03-31},
	journal = {Canadian Journal of Statistics},
	author = {Sinha, Sanjoy K. and Rao, J. N. K.},
	month = sep,
	year = {2009},
	pages = {381--399},
	file = {Sinha and Rao - 2009 - Robust small area estimation.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/438FN53B/Sinha and Rao - 2009 - Robust small area estimation.pdf:application/pdf},
}

@article{gao_improving_2021,
	title = {Improving multilevel regression and poststratification with structured priors},
	volume = {forthcoming},
	abstract = {A central theme in the ﬁeld of survey statistics is estimating populationlevel quantities through data coming from potentially non-representative samples of the population. Multilevel regression and poststratiﬁcation (MRP), a modelbased approach, is gaining traction against the traditional weighted approach for survey estimates. MRP estimates are susceptible to bias if there is an underlying structure that the methodology does not capture. This work aims to provide a new framework for specifying structured prior distributions that lead to bias reduction in MRP estimates. We use simulation studies to explore the beneﬁt of these prior distributions and demonstrate their eﬃcacy on non-representative US survey data. We show that structured prior distributions oﬀer absolute bias reduction and variance reduction for posterior MRP estimates in a large variety of data regimes.},
	journal = {Bayesian Analysis},
	author = {Gao, Yuxiang and Kennedy, Lauren and Simpson, Daniel and Gelman, Andrew},
	year = {2021},
	keywords = {Statistics - Methodology},
	file = {Gao et al. - 2020 - Improving multilevel regression and poststratifica.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/EPQY3R4H/Gao et al. - 2020 - Improving multilevel regression and poststratifica.pdf:application/pdf},
}

@article{pfeffermann_new_2013,
	title = {New {Important} {Developments} in {Small} {Area} {Estimation}},
	volume = {28},
	abstract = {The problem of small area estimation (SAE) is how to produce reliable estimates of characteristics of interest such as means, counts, quantiles, etc., for areas or domains for which only small samples or no samples are available, and how to assess their precision. The purpose of this paper is to review and discuss some of the new important developments in small area estimation methods. Rao [Small Area Estimation (2003)] wrote a very comprehensive book, which covers all the main developments in this topic until that time. A few review papers have been written after 2003, but they are limited in scope. Hence, the focus of this review is on new developments in the last 7–8 years, but to make the review more self-contained, I also mention shortly some of the older developments. The review covers both designbased and model-dependent methods, with the latter methods further classiﬁed into frequentist and Bayesian methods. The style of the paper is similar to the style of my previous review on SAE published in 2002, explaining the new problems investigated and describing the proposed solutions, but without dwelling on theoretical details, which can be found in the original articles. I hope that this paper will be useful both to researchers who like to learn more on the research carried out in SAE and to practitioners who might be interested in the application of the new methods.},
	number = {1},
	journal = {Statistical Science},
	author = {Pfeffermann, Danny},
	year = {2013},
	keywords = {sae},
	pages = {40--68},
	file = {Pfeffermann - 2013 - New Important Developments in Small Area Estimatio.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/M422Y2GX/Pfeffermann - 2013 - New Important Developments in Small Area Estimatio.pdf:application/pdf},
}

@article{simpson_penalising_2017,
	title = {Penalising {Model} {Component} {Complexity}: {A} {Principled}, {Practical} {Approach} to {Constructing} {Priors}},
	volume = {32},
	issn = {0883-4237},
	shorttitle = {Penalising {Model} {Component} {Complexity}},
	url = {https://projecteuclid.org/journals/statistical-science/volume-32/issue-1/Penalising-Model-Component-Complexity--A-Principled-Practical-Approach-to/10.1214/16-STS576.full},
	doi = {10.1214/16-STS576},
	abstract = {In this paper, we introduce a new concept for constructing prior distributions. We exploit the natural nested structure inherent to many model components, which deﬁnes the model component to be a ﬂexible extension of a base model. Proper priors are deﬁned to penalise the complexity induced by deviating from the simpler base model and are formulated after the input of a user-deﬁned scaling parameter for that model component, both in the univariate and the multivariate case. These priors are invariant to reparameterisations, have a natural connection to Jeffreys’ priors, are designed to support Occam’s razor and seem to have excellent robustness properties, all which are highly desirable and allow us to use this approach to deﬁne default prior distributions. Through examples and theoretical results, we demonstrate the appropriateness of this approach and how it can be applied in various situations.},
	language = {en},
	number = {1},
	urldate = {2021-04-01},
	journal = {Statistical Science},
	author = {Simpson, Daniel and Rue, Håvard and Riebler, Andrea and Martins, Thiago G. and Sørbye, Sigrunn H.},
	month = feb,
	year = {2017},
	keywords = {bayesian statistics, priors},
	file = {Simpson et al. - 2017 - Penalising Model Component Complexity A Principle.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/LB5SUQDN/Simpson et al. - 2017 - Penalising Model Component Complexity A Principle.pdf:application/pdf},
}

@article{jiang_robust_2020,
	title = {Robust {Small} {Area} {Estimation}: {An} {Overview}},
	volume = {7},
	abstract = {A small area typically refers to a subpopulation or domain of interest for which a reliable direct estimate, based only on the domain-specific sample, cannot be produced due to small sample size in the domain. While traditional small area methods and models are widely used nowadays, there have also been much work and interest in robust statistical inference for small area estimation (SAE). We survey this work and provide a comprehensive review here. We begin with a brief review of the traditional SAE methods. We then discuss SAE methods that are developed under weaker assumptions and SAE methods that are robust in certain ways, such as in terms of outliers or model failure. Our discussion also includes topics such as nonparametric SAE methods, Bayesian approaches, model selection and diagnostics, and missing data. A brief review of software packages available for implementing robust SAE methods is also given.},
	language = {en},
	number = {1},
	journal = {Annual Review of Statistics and Its Application},
	author = {Jiang, Jiming and Rao, J. Sunil},
	year = {2020},
	keywords = {sae},
	pages = {337--360},
	file = {Jiang and Rao - 2020 - Robust Small Area Estimation An Overview.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/TZSCMEV6/Jiang and Rao - 2020 - Robust Small Area Estimation An Overview.pdf:application/pdf;Jiang and Rao - 2020 - Robust Small Area Estimation An Overview.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/TQMZL73C/Jiang and Rao - 2020 - Robust Small Area Estimation An Overview.pdf:application/pdf},
}

@article{toto_bayesian_2010,
	title = {A {Bayesian} predictive inference for small area means incorporating covariates and sampling weights},
	volume = {140},
	issn = {03783758},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0378375810001643},
	doi = {10.1016/j.jspi.2010.03.043},
	abstract = {The main goal in small area estimation is to use models to ‘borrow strength’ from the ensemble because the direct estimates of small area parameters are generally unreliable. However, model-based estimates from the small areas do not usually match the value of the single estimate for the large area. Benchmarking is done by applying a constraint, internally or externally, to ensure that the ‘total’ of the small areas matches the ‘grand total’. This is particularly useful because it is difﬁcult to check model assumptions owing to the sparseness of the data. We use a Bayesian nested error regression model, which incorporates unit-level covariates and sampling weights, to develop a method to internally benchmark the ﬁnite population means of small areas. We use two examples to illustrate our method. We also perform a simulation study to further assess the properties of our method.},
	language = {en},
	number = {11},
	urldate = {2021-04-02},
	journal = {Journal of Statistical Planning and Inference},
	author = {Toto, Ma. Criselda S. and Nandram, Balgobin},
	month = nov,
	year = {2010},
	pages = {2963--2979},
	file = {Toto and Nandram - 2010 - A Bayesian predictive inference for small area mea.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/CJCT9UBC/Toto and Nandram - 2010 - A Bayesian predictive inference for small area mea.pdf:application/pdf},
}

@article{gelman_prior_2017,
	title = {The {Prior} {Can} {Often} {Only} {Be} {Understood} in the {Context} of the {Likelihood}},
	volume = {19},
	issn = {1099-4300},
	url = {http://www.mdpi.com/1099-4300/19/10/555},
	doi = {10.3390/e19100555},
	abstract = {A key sticking point of Bayesian analysis is the choice of prior distribution, and there is a vast literature on potential defaults including uniform priors, Jeffreys’ priors, reference priors, maximum entropy priors, and weakly informative priors. These methods, however, often manifest a key conceptual tension in prior modeling: a model encoding true prior information should be chosen without reference to the model of the measurement process, but almost all common prior modeling techniques are implicitly motivated by a reference likelihood. In this paper we resolve this apparent paradox by placing the choice of prior into the context of the entire Bayesian analysis, from inference to prediction to model evaluation.},
	language = {en},
	number = {10},
	urldate = {2021-04-02},
	journal = {Entropy},
	author = {Gelman, Andrew and Simpson, Daniel and Betancourt, Michael},
	month = oct,
	year = {2017},
	pages = {555},
	file = {Gelman et al. - 2017 - The Prior Can Often Only Be Understood in the Cont.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/2TZC4VSM/Gelman et al. - 2017 - The Prior Can Often Only Be Understood in the Cont.pdf:application/pdf},
}

@misc{betancourt_towards_2020,
	title = {Towards {A} {Principled} {Bayesian} {Workflow}},
	url = {https://betanalpha.github.io/assets/case_studies/principled_bayesian_workflow.html},
	language = {en},
	urldate = {2021-08-05},
	author = {Betancourt, Michael},
	month = apr,
	year = {2020},
	keywords = {bayesian statistics},
	file = {Towards A Principled Bayesian Workflow.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/8GKK2H2Q/Towards A Principled Bayesian Workflow.pdf:application/pdf},
}

@misc{inegi_modulo_2011,
	title = {Módulo de {Condiciones} {Socioeconómicas}. {Encuesta} {Nacional} de {Ingresos} y {Gastos} de los {Hogares} 2010. {Diseño} muestral},
	url = {https://www.inegi.org.mx/app/biblioteca/ficha.html?upc=702825002426},
	language = {es},
	urldate = {2021-08-09},
	author = {{INEGI}},
	year = {2011},
	keywords = {data sets},
	file = {Módulo de Condiciones Socioeconómicas. Encuesta Na.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/QA89LXSV/Módulo de Condiciones Socioeconómicas. Encuesta Na.pdf:application/pdf},
}

@article{zhang_bayesian_2020,
	title = {Bayesian {Regression} {Using} a {Prior} on the {Model} {Fit}: {The} {R2}-{D2} {Shrinkage} {Prior}},
	volume = {forthcoming},
	abstract = {Prior distributions for high-dimensional linear regression require specifying a joint distribution for the unobserved regression coeﬃcients, which is inherently diﬃcult. We instead propose a new class of shrinkage priors for linear regression via specifying a prior ﬁrst on the model ﬁt, in particular, the coeﬃcient of determination, and then distributing through to the coeﬃcients in a novel way. The proposed method compares favorably to previous approaches in terms of both concentration around the origin and tail behavior, which leads to improved performance both in posterior contraction and in empirical performance. The limiting behavior of the proposed prior is 1/x, both around the origin and in the tails. This behavior is optimal in the sense that it simultaneously lies on the boundary of being an improper prior both in the tails and around the origin. None of the existing shrinkage priors obtain this behavior in both regions simultaneously. We also demonstrate that our proposed prior leads to the same near-minimax posterior contraction rate as the spike-and-slab prior.},
	journal = {Journal of the American Statistical Association},
	author = {Zhang, Yan Dora and Naughton, Brian P. and Bondell, Howard D. and Reich, Brian J.},
	year = {2020},
	keywords = {bayesian statistics, priors},
	file = {Zhang et al. - 2020 - Bayesian Regression Using a Prior on the Model Fit.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/8D86WVGN/Zhang et al. - 2020 - Bayesian Regression Using a Prior on the Model Fit.pdf:application/pdf},
}

@article{yao_using_2018,
	title = {Using {Stacking} to {Average} {Bayesian} {Predictive} {Distributions}},
	volume = {13},
	abstract = {Bayesian model averaging is ﬂawed in the M-open setting in which the true data-generating process is not one of the candidate models being ﬁt. We take the idea of stacking from the point estimation literature and generalize to the combination of predictive distributions. We extend the utility function to any proper scoring rule and use Pareto smoothed importance sampling to eﬃciently compute the required leave-one-out posterior distributions. We compare stacking of predictive distributions to several alternatives: stacking of means, Bayesian model averaging (BMA), Pseudo-BMA, and a variant of Pseudo-BMA that is stabilized using the Bayesian bootstrap. Based on simulations and real-data applications, we recommend stacking of predictive distributions, with bootstrapped-Pseudo-BMA as an approximate alternative when computation cost is an issue.},
	number = {3},
	journal = {Bayesian Analysis},
	author = {Yao, Yuling and Vehtari, Aki and Simpson, Daniel and Gelman, Andrew},
	year = {2018},
	keywords = {bayesian statistics, model stacking},
	pages = {917--1007},
	file = {Yao et al. - 2018 - Using Stacking to Average Bayesian Predictive Dist.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/RI2NUFVA/Yao et al. - 2018 - Using Stacking to Average Bayesian Predictive Dist.pdf:application/pdf},
}

@inproceedings{feng_comparing_2015,
	address = {Fuzhou, China},
	title = {Comparing multilevel modelling and artificial neural networks in house price prediction},
	isbn = {978-1-4799-7748-2 978-1-4799-7749-9},
	url = {http://ieeexplore.ieee.org/document/7298035/},
	doi = {10.1109/ICSDM.2015.7298035},
	abstract = {Two advanced modelling approaches, Multi-Level Models and Artificial Neural Networks are employed to model house prices. These approaches and the standard Hedonic Price Model are compared in terms of predictive accuracy, capability to capture location information, and their explanatory power. These models are applied to 2001-2013 house prices in the Greater Bristol area, using secondary data from the Land Registry, the Population Census and Neighbourhood Statistics so that these models could be applied nationally. The results indicate that MLM offers good predictive accuracy with high explanatory power, especially if neighbourhood effects are explored at multiple spatial scales.},
	language = {en},
	urldate = {2021-04-24},
	booktitle = {2015 2nd {IEEE} {International} {Conference} on {Spatial} {Data} {Mining} and {Geographical} {Knowledge} {Services} ({ICSDM})},
	publisher = {IEEE},
	author = {Feng, Yingyu and Jones, Kelvyn},
	month = jul,
	year = {2015},
	keywords = {neural networks},
	pages = {108--114},
	file = {Feng and Jones - 2015 - Comparing multilevel modelling and artificial neur.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/4YWIVBQM/Feng and Jones - 2015 - Comparing multilevel modelling and artificial neur.pdf:application/pdf},
}

@article{piironen_sparsity_2017,
	title = {Sparsity information and regularization in the horseshoe and other shrinkage priors},
	volume = {11},
	abstract = {The horseshoe prior has proven to be a noteworthy alternative for sparse Bayesian estimation, but has previously suﬀered from two problems. First, there has been no systematic way of specifying a prior for the global shrinkage hyperparameter based on the prior information about the degree of sparsity in the parameter vector. Second, the horseshoe prior has the undesired property that there is no possibility of specifying separately information about sparsity and the amount of regularization for the largest coeﬃcients, which can be problematic with weakly identiﬁed parameters, such as the logistic regression coeﬃcients in the case of data separation. This paper proposes solutions to both of these problems. We introduce a concept of eﬀective number of nonzero parameters, show an intuitive way of formulating the prior for the global hyperparameter based on the sparsity assumptions, and argue that the previous default choices are dubious based on their tendency to favor solutions with more unshrunk parameters than we typically expect a priori. Moreover, we introduce a generalization to the horseshoe prior, called the regularized horseshoe, that allows us to specify a minimum level of regularization to the largest values. We show that the new prior can be considered as the continuous counterpart of the spike-and-slab prior with a ﬁnite slab width, whereas the original horseshoe resembles the spike-and-slab with an inﬁnitely wide slab. Numerical experiments on synthetic and real world data illustrate the beneﬁt of both of these theoretical advances.},
	number = {2},
	journal = {Electronic Journal of Statistics},
	author = {Piironen, Juho and Vehtari, Aki},
	year = {2017},
	keywords = {priors, regularization},
	pages = {5018--5051},
	file = {Piironen and Vehtari - 2017 - Sparsity information and regularization in the hor.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/V5TMUA74/Piironen and Vehtari - 2017 - Sparsity information and regularization in the hor.pdf:application/pdf},
}

@article{piironen_projective_2020,
	title = {Projective {Inference} in {High}-dimensional {Problems}: {Prediction} and {Feature} {Selection}},
	volume = {14},
	abstract = {This paper discusses predictive inference and feature selection for generalized linear models with scarce but high-dimensional data. We argue that in many cases one can beneﬁt from a decision theoretically justiﬁed two-stage approach: ﬁrst, construct a possibly non-sparse model that predicts well, and then ﬁnd a minimal subset of features that characterize the predictions. The model built in the ﬁrst step is referred to as the reference model and the operation during the latter step as predictive projection. The key characteristic of this approach is that it ﬁnds an excellent tradeoff between sparsity and predictive accuracy, and the gain comes from utilizing all available information including prior and that coming from the left out features. We review several methods that follow this principle and provide novel methodological contributions. We present a new projection technique that uniﬁes two existing techniques and is both accurate and fast to compute. We also propose a way of evaluating the feature selection process using fast leave-one-out cross-validation that allows for easy and intuitive model size selection. Furthermore, we prove a theorem that helps to understand the conditions under which the projective approach could be beneﬁcial. The beneﬁts are illustrated via several simulated and real world examples.},
	number = {1},
	journal = {Electronic Journal of Statistics},
	author = {Piironen, Juho and Paasiniemi, Markus and Vehtari, Aki},
	year = {2020},
	keywords = {machine learning, bayesian statistics, priors, model selection},
	pages = {2155 -- 2197},
	file = {Piironen et al. - 2020 - Projective Inference in High-dimensional Problems.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/PZKRLTTV/Piironen et al. - 2020 - Projective Inference in High-dimensional Problems.pdf:application/pdf},
}

@article{catalina_projection_2020,
	title = {Projection {Predictive} {Inference} for {Generalized} {Linear} and {Additive} {Multilevel} {Models}},
	url = {http://arxiv.org/abs/2010.06994},
	abstract = {Projection predictive inference is a decision theoretic Bayesian approach that decouples model estimation from decision making. Given a reference model previously built including all variables present in the data, projection predictive inference projects its posterior onto a constrained space of a subset of variables. Variable selection is then performed by sequentially adding relevant variables until predictive performance is satisfactory. Previously, projection predictive inference has been demonstrated only for generalized linear models (GLMs) and Gaussian processes (GPs) where it showed superior performance to competing variable selection procedures. In this work, we extend projection predictive inference to support variable and structure selection for generalized linear multilevel models (GLMMs) and generalized additive multilevel models (GAMMs). Our simulative and real-word experiments demonstrate that our method can drastically reduce the model complexity required to reach reference predictive performance and achieve good frequency properties.},
	language = {en},
	urldate = {2021-05-14},
	journal = {arXiv:2010.06994 [stat]},
	author = {Catalina, Alejandro and Bürkner, Paul-Christian and Vehtari, Aki},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.06994},
	keywords = {bayesian statistics, priors, model selection},
	file = {Catalina et al. - 2020 - Projection Predictive Inference for Generalized Li.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/NJVQ5HRJ/Catalina et al. - 2020 - Projection Predictive Inference for Generalized Li.pdf:application/pdf},
}

@article{kreutzmann_r_2019,
	title = {The {R} {Package} emdi for {Estimating} and {Mapping} {Regionally} {Disaggregated} {Indicators}},
	volume = {91},
	abstract = {The R package emdi enables the estimation of regionally disaggregated indicators using small area estimation methods and includes tools for processing, assessing, and presenting the results. The mean of the target variable, the quantiles of its distribution, the headcount ratio, the poverty gap, the Gini coeﬃcient, the quintile share ratio, and customized indicators are estimated using direct and model-based estimation with the empirical best predictor (Molina and Rao 2010). The user is assisted by automatic estimation of datadriven transformation parameters. Parametric and semi-parametric, wild bootstrap for mean squared error estimation are implemented with the latter oﬀering protection against possible misspeciﬁcation of the error distribution. Tools for (a) customized parallel computing, (b) model diagnostic analyses, (c) creating high quality maps and (d) exporting the results to Excel and OpenDocument Spreadsheets are included. The functionality of the package is illustrated with example data sets for estimating the Gini coeﬃcient and median income for districts in Austria.},
	number = {7},
	journal = {Journal of Statistical Software},
	author = {Kreutzmann, Ann-Kristin and Pannier, Sören and Rojas-Perilla, Natalia and Schmid, Timo and Templ, Matthias and Tzavidis, Nikos},
	year = {2019},
	file = {Kreutzmann et al. - 2019 - The R Package emdi for Estimating an.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/5VINYSEL/Kreutzmann et al. - 2019 - The R Package emdi for Estimating an.pdf:application/pdf},
}

@article{li_small_2019,
	title = {Small area estimation under transformed nested-error regression models},
	volume = {60},
	issn = {0932-5026, 1613-9798},
	url = {http://link.springer.com/10.1007/s00362-017-0879-7},
	doi = {10.1007/s00362-017-0879-7},
	abstract = {The empirical best linear unbiased prediction (EBLUP) based on the nested error regression model (Battese et al. in J Am Stat Assoc 83:28–36, 1988, NER) has been widely used for small area mean estimation. Its so-called optimality largely depends on the normality of the corresponding area level and unit level error terms. To allow departure from normality, we propose a transformed NER model with an invertible transformation, and employ the maximum likelihood method to estimate the underlying parameters of the transformed NER model. Motivated by Duan’s (J Am Stat Assoc 78:605–610, 1983) smearing estimator, we propose two small area mean estimators depending on whether all the population covariates or only the population covariate means are available in addition to sample covariates. We conduct two designbased simulation studies to investigate their ﬁnite-sample performance. The simulation results indicate that compared with existing methods such as the empirical best linear unbiased prediction, the proposed estimators are nearly the same reliable when the NER model is valid and become more reliable in general when the NER model is violated. In particular, our method does beneﬁt from incorporating auxiliary covariate information.},
	language = {en},
	number = {4},
	urldate = {2021-05-25},
	journal = {Statistical Papers},
	author = {Li, Huapeng and Liu, Yukun and Zhang, Riquan},
	month = aug,
	year = {2019},
	keywords = {sae, data transformation},
	pages = {1397--1418},
	file = {Li et al. - 2019 - Small area estimation under transformed nested-err.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/IULCMDJT/Li et al. - 2019 - Small area estimation under transformed nested-err.pdf:application/pdf},
}

@article{sugasawa_transforming_2017,
	title = {Transforming response values in small area prediction},
	volume = {114},
	issn = {01679473},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167947317300737},
	doi = {10.1016/j.csda.2017.03.017},
	abstract = {In real applications of small area estimation, one often encounters data with positive response values. The use of a parametric transformation for positive response values in the Fay–Herriot model is proposed for such a case. An asymptotically unbiased small area predictor is derived and a second-order unbiased estimator of the mean squared error is established using the parametric bootstrap. Through simulation studies, a finite sample performance of the proposed predictor and the MSE estimator is investigated. The methodology is also successfully applied to Japanese survey data.},
	language = {en},
	urldate = {2021-05-25},
	journal = {Computational Statistics \& Data Analysis},
	author = {Sugasawa, Shonosuke and Kubokawa, Tatsuya},
	month = oct,
	year = {2017},
	keywords = {sae, data transformation},
	pages = {47--60},
	file = {Sugasawa and Kubokawa - 2017 - Transforming response values in small area predict.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/2IT55WF4/Sugasawa and Kubokawa - 2017 - Transforming response values in small area predict.pdf:application/pdf},
}

@article{sugasawa_adaptively_2018,
	title = {Adaptively {Transformed} {Mixed} {Model} {Prediction} of {General} {Finite} {Population} {Parameters}},
	url = {http://arxiv.org/abs/1705.04136},
	abstract = {For estimating area-speciﬁc parameters (quantities) in a ﬁnite population, a mixed model prediction approach is attractive. However, this approach strongly depends on the normality assumption of the response values although we often encounter a non-normal case in practice. In such a case, transforming observations to make them suitable for normality assumption is a useful tool, but the problem of selecting suitable transformation still remains open. To overcome the diﬃculty, we here propose a new empirical best predicting method by using a parametric family of transformations to estimate a suitable transformation based on the data. We suggest a simple estimating method for transformation parameters based on the proﬁle likelihood function, which achieves consistency under some conditions on transformation functions. For measuring variability of point prediction, we construct an empirical Bayes conﬁdence interval of the population parameter of interest. Through simulation studies, we investigate numerical performance of the proposed methods. Finally, we apply the proposed method to synthetic income data in Spanish provinces in which the resulting estimates indicate that the commonly used log-transformation would not be appropriate.},
	language = {en},
	urldate = {2021-05-25},
	journal = {arXiv:1705.04136 [stat]},
	author = {Sugasawa, Shonosuke and Kubokawa, Tatsuya},
	month = jun,
	year = {2018},
	note = {arXiv: 1705.04136},
	keywords = {sae, data transformation},
	file = {Sugasawa and Kubokawa - 2018 - Adaptively Transformed Mixed Model Prediction of G.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/Z2PCEBQS/Sugasawa and Kubokawa - 2018 - Adaptively Transformed Mixed Model Prediction of G.pdf:application/pdf},
}

@article{griffin_hierarchical_2017,
	title = {Hierarchical {Shrinkage} {Priors} for {Regression} {Models}},
	volume = {12},
	issn = {1936-0975},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-12/issue-1/Hierarchical-Shrinkage-Priors-for-Regression-Models/10.1214/15-BA990.full},
	doi = {10.1214/15-BA990},
	abstract = {In some linear models, such as those with interactions, it is natural to include the relationship between the regression coeﬃcients in the analysis. In this paper, we consider how robust hierarchical continuous prior distributions can be used to express dependence between the size but not the sign of the regression coeﬃcients. For example, to include ideas of heredity in the analysis of linear models with interactions. We develop a simple method for controlling the shrinkage of regression eﬀects to zero at diﬀerent levels of the hierarchy by considering the behaviour of the continuous prior at zero. Applications to linear models with interactions and generalized additive models are used as illustrations.},
	language = {en},
	number = {1},
	urldate = {2021-05-29},
	journal = {Bayesian Analysis},
	author = {Griffin, Jim and Brown, Phil},
	month = mar,
	year = {2017},
	keywords = {priors, model selection},
	file = {Griffin and Brown - 2017 - Hierarchical Shrinkage Priors for Regression Model.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/G6MZPSXZ/Griffin and Brown - 2017 - Hierarchical Shrinkage Priors for Regression Model.pdf:application/pdf},
}

@article{carvalho_horseshoe_2010,
	title = {The horseshoe estimator for sparse signals},
	volume = {97},
	abstract = {This paper proposes a new approach to sparsity, called the horseshoe estimator, which arises from a prior based on multivariate-normal scale mixtures. We describe the estimator’s advantages over existing approaches, including its robustness, adaptivity to different sparsity patterns and analytical tractability. We prove two theorems: one that characterizes the horseshoe estimator’s tail robustness and the other that demonstrates a super-efﬁcient rate of convergence to the correct estimate of the sampling density in sparse situations. Finally, using both real and simulated data, we show that the horseshoe estimator corresponds quite closely to the answers obtained by Bayesian model averaging under a point-mass mixture prior.},
	language = {en},
	number = {2},
	journal = {Biometrika},
	author = {Carvalho, C. M. and Polson, N. G. and Scott, J. G.},
	year = {2010},
	pages = {465--480},
	file = {Carvalho et al. - 2010 - The horseshoe estimator for sparse signals.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/X6PQM33B/Carvalho et al. - 2010 - The horseshoe estimator for sparse signals.pdf:application/pdf},
}

@article{mitchell_bayesian_1988,
	title = {Bayesian {Variable} {Selection} in {Linear} {Regression}},
	volume = {83},
	number = {404},
	journal = {Journal of the American Statistical Association},
	author = {Mitchell, T. J. and Beauchamp, J. J.},
	year = {1988},
	keywords = {priors, model selection},
	pages = {1023--1032},
	file = {Mitchell and Beauchamp - 1988 - Bayesian Variable Selection in Linear Regression.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/C9PH8FCY/Mitchell and Beauchamp - 1988 - Bayesian Variable Selection in Linear Regression.pdf:application/pdf},
}

@article{tibshirani_regression_1996,
	title = {Regression {Shrinkage} and {Selection} {Via} the {Lasso}},
	volume = {58},
	number = {1},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Tibshirani, Robert},
	year = {1996},
	keywords = {model selection},
	pages = {267--288},
	file = {Tibshirani - 1996 - Regression Shrinkage and Selection Via the Lasso.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/MV4VTQE8/Tibshirani - 1996 - Regression Shrinkage and Selection Via the Lasso.pdf:application/pdf},
}

@techreport{morelli_hierarchical_2021,
	title = {Hierarchical {Bayesian} {Models}: {An} {Application} to {Small} {Area} {Estimation}},
	institution = {Free University Berlin},
	author = {Morelli, Flavio},
	year = {2021},
}

@article{burkner_brms_2017,
	title = {brms : {An} {R} {Package} for {Bayesian} {Multilevel} {Models} {Using} {Stan}},
	volume = {80},
	abstract = {The brms package implements Bayesian multilevel models in R using the probabilistic programming language Stan. A wide range of distributions and link functions are supported, allowing users to ﬁt – among others – linear, robust linear, binomial, Poisson, survival, ordinal, zero-inﬂated, hurdle, and even non-linear models all in a multilevel context. Further modeling options include autocorrelation of the response variable, user deﬁned covariance structures, censored data, as well as meta-analytic standard errors. Prior speciﬁcations are ﬂexible and explicitly encourage users to apply prior distributions that actually reﬂect their beliefs. In addition, model ﬁt can easily be assessed and compared with the Watanabe-Akaike information criterion and leave-one-out cross-validation.},
	number = {1},
	journal = {Journal of Statistical Software},
	author = {Bürkner, Paul-Christian},
	year = {2017},
	file = {Bürkner - 2017 - brms  An R Package for Bayesian Mul.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/9BQ66UKN/Bürkner - 2017 - brms  An R Package for Bayesian Mul.pdf:application/pdf},
}

@misc{stan_development_team_stan_2021,
	title = {Stan {Modeling} {Language} {Users} {Guide} and {Reference} {Manual}, 2.27},
	url = {https://mc-stan.org},
	urldate = {2021-08-05},
	author = {{Stan Development Team}},
	year = {2021},
}

@article{lewandowski_generating_2009,
	title = {Generating random correlation matrices based on vines and extended onion method},
	volume = {100},
	abstract = {We extend and improve two existing methods of generating random correlation matrices, the onion method of Ghosh and Henderson [S. Ghosh, S.G. Henderson, Behavior of the norta method for correlated random vector generation as the dimension increases, ACM Transactions on Modeling and Computer Simulation (TOMACS) 13 (3) (2003) 276–294] and the recently proposed method of Joe [H. Joe, Generating random correlation matrices based on partial correlations, Journal of Multivariate Analysis 97 (2006) 2177–2189] based on partial correlations. The latter is based on the so-called D-vine. We extend the methodology to any regular vine and study the relationship between the multiple correlation and partial correlations on a regular vine. We explain the onion method in terms of elliptical distributions and extend it to allow generating random correlation matrices from the same joint distribution as the vine method. The methods are compared in terms of time necessary to generate 5000 random correlation matrices of given dimensions.},
	number = {9},
	journal = {Journal of Multivariate Analysis},
	author = {Lewandowski, Daniel and Kurowicka, Dorota and Joe, Harry},
	year = {2009},
	keywords = {priors},
	pages = {1989--2001},
	file = {Lewandowski et al. - 2009 - Generating random correlation matrices based on vi.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/9WX4ZS49/Lewandowski et al. - 2009 - Generating random correlation matrices based on vi.pdf:application/pdf},
}

@incollection{neal_mcmc_2011,
	address = {Boca Raton, FL (US)},
	title = {{MCMC} using {Hamiltonian} dynamics},
	abstract = {Hamiltonian dynamics can be used to produce distant proposals for the Metropolis algorithm, thereby avoiding the slow exploration of the state space that results from the diffusive behaviour of simple random-walk proposals. Though originating in physics, Hamiltonian dynamics can be applied to most problems with continuous state spaces by simply introducing fictitious "momentum" variables. A key to its usefulness is that Hamiltonian dynamics preserves volume, and its trajectories can thus be used to define complex mappings without the need to account for a hard-to-compute Jacobian factor - a property that can be exactly maintained even when the dynamics is approximated by discretizing time. In this review, I discuss theoretical and practical aspects of Hamiltonian Monte Carlo, and present some of its variations, including using windows of states for deciding on acceptance or rejection, computing trajectories using fast approximations, tempering during the course of a trajectory to handle isolated modes, and short-cut methods that prevent useless trajectories from taking much computation time.},
	booktitle = {Handbook of {Markov} {Chain} {Monte} {Carlo}},
	publisher = {CRC Press},
	author = {Neal, Radford M.},
	editor = {Gelman, Andrew and Jones, Galin and Meng, Xiao-Li},
	year = {2011},
	keywords = {Physics - Computational Physics, Statistics - Computation},
	pages = {113--162},
	file = {Neal - 2011 - MCMC using Hamiltonian dynamics.pdf:/Users/flaviomejia/Documents/Books/Zotero/storage/QHL6YM49/Neal - 2011 - MCMC using Hamiltonian dynamics.pdf:application/pdf},
}

@book{gomez-rubio_bayesian_2020,
	address = {Boca Raton, FL (US)},
	title = {Bayesian inference with {INLA}},
	publisher = {CRC Press},
	author = {Gómez-Rubio, Virgilio},
	year = {2020},
}

@misc{united_nations_transforming_2015,
	title = {Transforming our world: the 2030 {Agenda} for {Sustainable} {Development}},
	url = {https://sdgs.un.org/2030agenda},
	urldate = {2021-10-03},
	author = {{United Nations}},
	year = {2015},
}
